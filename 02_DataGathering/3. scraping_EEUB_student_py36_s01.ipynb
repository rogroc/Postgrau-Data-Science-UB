{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>August 2014 - This notebook was created by [Oriol Pujol Vila](http://www.maia.ub.es/~oriol). Source and license info are in the folder.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data hunting and gathering (part 1 - student notebook)\n",
    "\n",
    "<img style = \"border-radius:20px;\" src = \"http://unadocenade.com/wp-content/uploads/2012/09/cavalls-de-valltorta.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents and Requirements \n",
    "\n",
    "**SESSION 1:**\n",
    "\n",
    "    + Warm-up and Crawling\n",
    "    + MongoDB basics (writting and reading)\n",
    "    + APIs:\n",
    "        + API wrappers\n",
    "        + direct API programming\n",
    "    \n",
    "SOFTWARE REQUIREMENTS FOR SESSION 1\n",
    "    \n",
    "    + mongoDB (>=4.0.9) (https://www.mongodb.com/download-center/community , donwload from the Server tab)\n",
    "    \n",
    "ADDITIONAL PYTHON LIBRARIES\n",
    "\n",
    "    + pymongo #pip install pymongo\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px;border-width:3px;border-color:darkred;font-family:Verdana,sans-serif;font-size:16px;\">\n",
    "**DISCLAIMER AND USER AGREEMENT:** Ensure you are allowed to use these tools for retrieving data and be respectful with web pages and apps. Ethical use of these tools is mandatory. The content provided by this notebook is for educational purposes only. \n",
    "<p>\n",
    "\n",
    "THE NOTEBOOK/SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE NOTEBOOK CONTENTS OR SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is the basis of this course. Although we usually find it in well structured formats such as a spreadsheet resulting from our last experiment, or the collection of company records in a classical relational database, with the advent of internet new information sources have to be taken into account. However, these new sources are home of unstructured data. In this lecture several methods for retrieving data and storing it are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first introduce the big picture guiding this lecture. Whenever we want to retrieve data from a web site we should ask first if the web site is providing a simple way for that purpose. Many large sites such as google, facebook, twitter, etc, provide a **Application Programming Interface (API)** that can make data hunting easier. However, most of web sites do not have this interface. Even more, an API may not provide the desired information. In those cases we have to use **scraping** techniques. This means dealing with the raw information as it is provided to the web browser and code our data finding methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"border-radius:20px;\" src=\"./files/big_picture.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start connecting to the net and checking out how to retrieve a basic page. We will start using `urllib.request` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "source = urlopen(\"http://www.google.com/\")\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check what is in\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hurray we got a socket. An all sockets behave like files, so let us go read() the \"file\"\n",
    "something = source.read().decode('latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check on something\n",
    "print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What!!!!\n",
    "#Let us read more\n",
    "print(source.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ooooppss nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, hands on!!! Some first warm up exercises:\n",
    "\n",
    "<div class = \"alert alert-success\" style = \"border-radius:10px;border-width:3px;border-color:darkgreen;font-family:Verdana,sans-serif;font-size:16px;\">**WARM UP EXERCISES**\n",
    "<ol>\n",
    "<li>Is there the word python in pyladies.org?</li>\n",
    "<li>Does http://google.com contain an image? (hint: < img > TAG )</li>\n",
    "<li>What are the first ten characters of python.org?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX 1. Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX 2. Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX 3. Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are retrieving data from an URL! So we are done! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling and Scraping\n",
    "\n",
    "Scraping and **crawling** are two very related techniques. While scraping is used for retrieving data from a web page, crawling is used to retrieve the web pages. Scraping and crawling are found at the core of search engines. Scraping is used to get keywords, analyze, and extract useful information from the web pages so that given a user query it may return related results. On the other hand, crawling allows to retrieve the actual pages and uses scraping to get the links in each web site. This allows to create a graph of the connection among web sites and this information can be used to order the results of a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we might want not only to get data from a single page but probably retrieve from several related pages. In those cases crawling is the way to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px;border-width:3px;border-color:darkgreen;font-family:Verdana,sans-serif;font-size:16px;\">**WARM-UP PROJECT:** Let us build a very simple spider. The basic functionality of an spider is to crawl and store all the data in web pages. In this simple project we will take care of single site. \n",
    "\n",
    "<ol>\n",
    "<li>A crawler must recognize the links to crawl. Take a minute and think how to retrieve the links of a web site.</li>\n",
    "<li>Let us start the project by creating a Spider class. The constructor will have the following parameters: starting_url, crawl_domain, and max_iter. crawl_domain will be the domain that validates if an absolute link will be considered or not. max_iter is the maximum amount of web items to crawl.</li>\n",
    "<li>The main method can be Spider.run(). Enumerate the big functionalities/building blocks of the crawler.</li>\n",
    "</ol>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def getLinks(html, max_links=10):\n",
    "    url = []\n",
    "    cursor = 0\n",
    "    nlinks=0\n",
    "    while (cursor>=0 and nlinks<max_links):\n",
    "        start_link = html.find(\"a href\",cursor)\n",
    "        if start_link==-1:\n",
    "            return url\n",
    "        start_quote = html.find('\"', start_link)\n",
    "        end_quote = html.find('\"', start_quote + 1)\n",
    "        url.append(html[start_quote + 1: end_quote])\n",
    "        cursor = end_quote+1\n",
    "        nlinks = nlinks +1\n",
    "    return url\n",
    "\n",
    "class Spider:\n",
    "    def __init__(self,starting_url,crawl_domain,max_iter):\n",
    "        self.crawl_domain = crawl_domain\n",
    "        self.max_iter = max_iter\n",
    "        self.links_to_crawl=[]\n",
    "        self.links_to_crawl.append(starting_url)\n",
    "        self.links_visited=[]\n",
    "        self.collection=[]\n",
    "        \n",
    "    def retrieveHtml(self):\n",
    "        try:\n",
    "            socket = urlopen(self.url);\n",
    "            self.html = socket.read().decode('latin-1')\n",
    "            return 0\n",
    "        except HTTPError:\n",
    "            # Most probably an url not found 404, possibly due to malformating of the links in retrieveAndValidateLinks\n",
    "            return -1\n",
    "             \n",
    "    def run(self):\n",
    "        while (len(self.links_to_crawl)>0 and len(self.collection)<self.max_iter):\n",
    "            self.url = self.links_to_crawl.pop(0)\n",
    "            print (self.links_to_crawl)\n",
    "            self.links_visited.append(self.url)\n",
    "            if self.retrieveHtml()>=0:\n",
    "                self.storeHtml()\n",
    "                self.retrieveAndValidateLinks()\n",
    "    \n",
    "    def retrieveAndValidateLinks(self):\n",
    "        tmpList=[]\n",
    "        items = getLinks(self.html)\n",
    "        # Check the validity of a link\n",
    "        for item in items:\n",
    "            item = item.strip('\"')\n",
    "            if self.crawl_domain in item:\n",
    "                tmpList.append(item)\n",
    "            if not(\":\") in item: #Take care of http:// https:// and mailto:\n",
    "                tmpList.append(self.crawl_domain+item)\n",
    "        # Check that the link has not been previously retrieved or is currently on the links_to_crawl list\n",
    "        for item in tmpList:\n",
    "            if item not in self.links_visited:\n",
    "                if item not in self.links_to_crawl:\n",
    "                    self.links_to_crawl.append(item)\n",
    "                    print ('Adding: '+item)\n",
    "                \n",
    "    def storeHtml(self):\n",
    "        doc = {}\n",
    "        doc['url'] = self.url\n",
    "        doc['date'] = time.strftime(\"%d/%m/%Y\")\n",
    "        doc['html'] = self.html\n",
    "        self.collection.append(doc)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us validate the crawler with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider = Spider('http://www.ub.edu/datascience/postgraduate/','http://www.ub.edu/datascience/postgraduate/',20)\n",
    "spider.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many elements does our colletion have?\n",
    "len(spider.collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider.collection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the urls retreived\n",
    "[spider.collection[i]['url'] for i in range(len(spider.collection))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go for a more complex web site. Run the code on http://hunch.net (a machine learning blog by John Langford)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider = Spider('http://hunch.net','http://hunch.net/',10)\n",
    "spider.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And check the urls retrieved\n",
    "[spider.collection[i]['url'] for i in range(len(spider.collection))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the simple crawler more or less works as expected. There are still many functionalities to work on , such as valid domains, valid urls, etc. One important issue to consider is **persistence**, or how to store the data retrieved for further analysis. In this basic scraping tutorial we us MongoDB as a Non-SQL database for persistence purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to MongoDB\n",
    "<small>This introduction is partially inspired on the notes of Alberto Negron's [blog](http://altons.github.io/python/2013/01/21/gentle-introduction-to-mongodb-using-pymongo/)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB is a document-oriented database, part of the NoSQL family of database systems. MongoDB stores structured data as JSON-like structures. From a pythonic point of view it is like storing dictionary data structures. One of its main feature is its schema-less feature, i.e. it supports dynamic schemas. A schema in a relational database informally referst to the structure of the data it stores, i.e. what kind of data, which tables, which relations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us change the Spider class to support MongoDB persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let us configure the MongoDB system.\n",
    "\n",
    "+ Download mongoDB.\n",
    "+ Rename the folder to mongodb.\n",
    "+ Add a directory data and log in your working project directory.\n",
    "+ Check that the server works \n",
    "\n",
    "        `mongod --dbpath . --nojournal` (use `./mongod --dbpath . --nojournal &` in linux based systems)\n",
    "        \n",
    "+ Check the connection to the server: in another terminal write mongo, check that it does not raise any error and exit the console.\n",
    "+ Close the mongo daemon (mongod). You may have to kill mongod with kill -9 and remove the lock on the daemon, mongod.lock.\n",
    "+ Let us configure a little the data base by configuring the path of the data storage and log files. Create a [mongo.conf](./mongodb/data/mongo.conf) file such as the one provided  and start the server using the following command:\n",
    "\n",
    "        mongod --config=./mongodb/data/mongo.conf --nojournal &\n",
    "        \n",
    "+ Bonus: we can check the database status using  http://127.0.0.1:27017/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to a MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connection to Mongo DB\n",
    "try:\n",
    "    conn=pymongo.MongoClient()\n",
    "    print (\"Connected successfully!!!\")\n",
    "except pymongo.errors.ConnectionFailure as e:\n",
    "    print (\"Could not connect to MongoDB: %s\" % e )\n",
    "conn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "conn = pymongo.MongoClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **create** a database using attribute access <span style = \"font-family:Courier;\"> db = conn.name_db</span> or dictionary acces <span style = \"font-family:Courier;\"> db = conn[name_db]</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a database using db = conn.name_db or dictionary access db = conn['name_db']\n",
    "db = conn['datascienceUB_Octubre_2018']\n",
    "print (db)\n",
    "conn.list_database_names()\n",
    "#Empty databases do not show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A database stores a **collection**. A collection is a group of documents stored in MongoDB, and can be thought of as the equivalent of a table in a relational database. Getting a collection in PyMongo works the same as getting a database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['Hola']\n",
    "db.list_collection_names()\n",
    "#Empty collections do not show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The database has a collection, thus ...\n",
    "conn.list_database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB stores structured data as JSON-like (JavaScript Object Notation) documents, using dynamic schemas (called BSON), rather than predefined schemas. An element of data is called a document, and documents are stored in collections. One collection may have any number of documents.\n",
    "\n",
    "Compared to relational databases, we could say collections are like tables, and documents are like records. But there is one big difference: every record in a table has the same fields (with, usually, differing values) in the same order, while each document in a collection can have completely different fields from the other documents.\n",
    "\n",
    "All you really need to know when you're using Python, however, is that documents are Python dictionaries that can have strings as keys and can contain various primitive types (int, float,unicode, datetime) as well as other documents (Python dicts) and arrays (Python lists).\n",
    "\n",
    "To insert some data into MongoDB, all we need to do is create a dict and call .insert() on the collection object. Let us exemplify this process by downloading an url and storing it in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import time\n",
    "# dd/mm/yyyy format\n",
    "print (time.strftime(\"%d/%m/%Y\"))\n",
    "url = 'http://www.ub.edu/datascience/postgraduate/'\n",
    "html = urlopen(url).read().decode('latin-1')\n",
    "\n",
    "#Create a dictionary/document to store\n",
    "doc = {}\n",
    "doc['url'] = url\n",
    "doc['date'] = time.strftime(\"%d/%m/%Y\")\n",
    "doc['html'] = html\n",
    "doc['adios'] = 'esto es otra prueba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the document in the collection\n",
    "collection.insert_one(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that we have a non empty collection.\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, we have databases containing collections. A collection is made up of documents. Each document is made up of fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.find_one() #Returns one random document in the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more than a single document as the result of a query we use the find() method. find() returns a Cursor instance, which allows us to iterate over all matching documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in collection.find():\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving filtered documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very naive way to filter is to run on all documents and filter the resulting documents. Thus, a programatic way to filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in collection.find():\n",
    "    try:\n",
    "        if \"datascience\" in d[\"url\"]:\n",
    "            print(d[\"url\"])\n",
    "    except KeyError:\n",
    "        print(\"ERROR\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we can directly use .find() for querying in pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in collection.find({\"atributo\":\"valor del atributo\"}):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that it finds exact matches. Operations include *gt* (greater than), *gte* (greater than equal), *lt* (lesser than), *lte* (lesser than equal), *ne* (not equal), *nin* (not in a list), *regex* (regular expression), *exists*, *not*, *or*, *and*, etc. Let us see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.find({\"date\":{\"$gte\":\"01/01/2014\"}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the most porwerful way to directly filter is to us **regular expressions** as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substring = \"datascience\"\n",
    "reg = substring\n",
    "collection.find({\"html\":{\"$regex\":reg}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in collection.find({\"html\":{\"$regex\":\"datascience\"}}):\n",
    "    print (item['html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are usually not the answer due to the fragility of html pages on the internet today -- common mistakes like missing end tags, mismatched tags, forgetting to close an attribute quote, would all derail a perfectly good regular expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally close the connection with the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Finishing the warm up project with MongoDB storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import time\n",
    "#Import pymongo\n",
    "import pymongo\n",
    "\n",
    "\n",
    "def getLinks(html, max_links=10):\n",
    "    url = []\n",
    "    cursor = 0\n",
    "    nlinks=0\n",
    "    while (cursor>=0 and nlinks<max_links):\n",
    "        start_link = html.find(\"a href\",cursor)\n",
    "        if start_link==-1:\n",
    "            return url\n",
    "        start_quote = html.find('\"', start_link)\n",
    "        end_quote = html.find('\"', start_quote + 1)\n",
    "        url.append(html[start_quote + 1: end_quote])\n",
    "        cursor = end_quote+1\n",
    "        nlinks = nlinks +1\n",
    "    return url\n",
    "\n",
    "class Spider:\n",
    "    def __init__(self,starting_url,crawl_domain,max_iter):\n",
    "        self.crawl_domain = crawl_domain\n",
    "        self.max_iter = max_iter\n",
    "        self.links_to_crawl=[]\n",
    "        self.links_to_crawl.append(starting_url)\n",
    "        self.links_visited=[]\n",
    "        self.collection=[]\n",
    "        # Create the connection to MongoDB\n",
    "        try:\n",
    "            self.conn=pymongo.MongoClient()\n",
    "            print (\"Connection to Mongo Daemon successful!!!\")\n",
    "        except pymongo.errors.ConnectionFailure as e:\n",
    "            print (\"Could not connect to MongoDB: %s\" % e )\n",
    "        self.db = conn['crawlerDB2019']\n",
    "        self.collection = self.db[starting_url+'DB']\n",
    "\n",
    "        \n",
    "    def retrieveHtml(self):\n",
    "        try:\n",
    "            socket = urlopen(self.url);\n",
    "            self.html = socket.read().decode('latin-1')\n",
    "            return 0\n",
    "        except HTTPError:\n",
    "            # Most probably an url not found 404, possibly due to malformating of the links in retrieveAndValidateLinks\n",
    "            return -1\n",
    "             \n",
    "    def run(self):\n",
    "        #Change the count on the collection\n",
    "        while (len(self.links_to_crawl)>0 and self.collection.count()<self.max_iter):\n",
    "            self.url = self.links_to_crawl.pop(0)\n",
    "            print (self.links_to_crawl)\n",
    "            self.links_visited.append(self.url)\n",
    "            if self.retrieveHtml()>=0:\n",
    "                self.storeHtml()\n",
    "                self.retrieveAndValidateLinks()\n",
    "        self.conn.close()\n",
    "    \n",
    "    def retrieveAndValidateLinks(self):\n",
    "        tmpList=[]\n",
    "        items = getLinks(self.html,max_links=50)\n",
    "        # Check the validity of a link\n",
    "        for item in items:\n",
    "            item = item.strip('\"')\n",
    "            if '.pdf' not in item:\n",
    "                if self.crawl_domain in item:\n",
    "                    tmpList.append(item)\n",
    "                else:\n",
    "                    if not(\":\") in item: #Take care of http:// https:// and mailto:\n",
    "                        tmpList.append(self.crawl_domain+item)\n",
    "        # Check that the link has not been previously retrieved or is currently on the links_to_crawl list\n",
    "        for item in tmpList:\n",
    "            if item not in self.links_visited:\n",
    "                if item not in self.links_to_crawl:\n",
    "                    self.links_to_crawl.append(item)\n",
    "                    print ('Adding: '+item)\n",
    "                \n",
    "    def storeHtml(self):\n",
    "        doc = {}\n",
    "        doc['url'] = self.url\n",
    "        doc['date'] = time.strftime(\"%d/%m/%Y\")\n",
    "        doc['html'] = self.html\n",
    "        #Insert in the collection\n",
    "        self.collection.insert_one(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider = Spider('http://hunch.net','http://hunch.net/',20)\n",
    "spider.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymongo.MongoClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (conn.database_names())\n",
    "db = conn['crawlerDB2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['http://hunch.netDB']\n",
    "collection.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in collection.find():\n",
    "    print (doc['url'])\n",
    "    print (doc['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;font-size:16px;\">\n",
    "\n",
    "**PROS and CONS:**\n",
    "<p>\n",
    "**MongoDB** querying is powerful but based on basic string operations. This actually tells us that storing full HTML pages is not going to be effiecient for retrieval. Actually, we will see that it is important to break the information in the pieces we really want. However, this is a good starting point before a post processing if we are not sure what we are going to do with the data or further scraping is going to take long. </p>\n",
    "</div>\n",
    "\n",
    "In the next section we will see more efficient ways of dealing with web based data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;font-size:16px;\">\n",
    "\n",
    "**URLLIB** is good for getting simple things. In the end you end up with a large HTML string you want to do something on it. \n",
    "So the next thing you want to do is to parse data. But you want to do it in the same way you do when you interact with the web page. You see a menu, a frame on the left side, a nice colorful block where the price for your flight is. So **you want to parse data the way you see data in the webpage so that you can target it**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the API\n",
    "\n",
    "Recall the **big picture**. If we are targeting for specific data we could check if the web site has a programatic interface for querying. If it has we can use it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"border-radius:20px;\" src=\"./files/big_picture.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard way for programatically communicating with a web service is using the API (Application Programing Interface) whenever it is provided. \n",
    "\n",
    "\n",
    "For example, Twitter provides several APIs. The two most important ones are the RESTful API for static queries (e.g. user's friends and followers, check timelines, etc) and the Streaming API for retrieving live data. The REST API identifies Twitter applications and users using OAuth; responses are available in JSON. The Streaming API should not need authentication.\n",
    "\n",
    "Ex. \n",
    "\n",
    "https://api.twitter.com/oauth/authenticate?oauth_token=XXXXXXXXXXXXXX\n",
    "\n",
    "https://api.twitter.com/1.1/followers/ids.json?cursor=-1&screen_name=my_user_name&count=5000\n",
    "\n",
    "Building these queries is not always easy, thus we may use a wrapper around the API. This is what, for example, **tweepy** does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the API with authentification (needed for the RESTful API)\n",
    "\n",
    "From wikipedia:\n",
    "\n",
    ">\"Web service APIs that adhere to the architectural constraints are called RESTful. HTTP based RESTful APIs are defined with these aspects:\n",
    "\n",
    "> <ul><li>base URI (Uniform Resource Identifier), such as http://example.com/resources/\n",
    "<li>an Internet media type for the data. This is often JSON but can be any other valid Internet media type (e.g. XML, Atom, microformats, images, etc.)</li>\n",
    "<li>standard HTTP methods (e.g., GET [retrieve], PUT[idempotent update/create], POST[update/create], or DELETE)</li>\n",
    "<li>hypertext links to reference state</li>\n",
    "<li>hypertext links to reference related resources\"</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Making your own API Interface using Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using standard tools we can directly attack the API to build queries and use request. \n",
    "\n",
    "Reference for the NASA API: https://api.nasa.gov/api.html#imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From NASA web-site:**\n",
    "    \n",
    "\n",
    "This endpoint retrieves the Landsat 8 image for the supplied location and date. The response will include the date and URL to the image that is closest to the supplied date. The requested resource may not be available for the exact date in the request. You can retrieve a list of available resources through the assets endpoint.\n",
    "\n",
    "The cloud score is an optional calculation that returns the percentage of the queried image that is covered by clouds. If False is supplied to the cloud_score parameter, then no keypair is returned. If True is supplied, then a keypair will always be returned, even if the backend algorithm is not able to calculate a score. Note that this is a rough calculation, mainly used to filter out exceedingly cloudy images.\n",
    "\n",
    "HTTP REQUEST\n",
    "GET https://api.nasa.gov/planetary/earth/imagery\n",
    "\n",
    "QUERY PARAMETERS\n",
    "+ Parameter\tType\tDefault\tDescription\n",
    "+ lat\tfloat\tn/a\tLatitude\n",
    "+ lon\tfloat\tn/a\tLongitude\n",
    "+ dim\tfloat\t0.025\twidth and height of image in degrees\n",
    "+ date\tYYYY-MM-DD\ttoday\tdate of image; if not supplied, then the most recent image (i.e., closest to today) is returned\n",
    "+ cloud_score\tbool\tFalse\tcalculate the percentage of the image covered by clouds\n",
    "+ api_key\tstring\tDEMO_KEY\tapi.nasa.gov key for expanded usage\n",
    "\n",
    "EXAMPLE QUERY\n",
    "https://api.nasa.gov/planetary/earth/imagery?lon=100.75&lat=1.5&date=2014-02-01&cloud_score=True&api_key=DEMO_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo Key is only for few uses so we might need to ask for a key ourselves at api.nasa.gov \n",
    "\n",
    "Let's start with the most simple application. Just replicate the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WARNING: YOU WILL NEED AN ID!!!! ##########\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://api.nasa.gov/planetary/earth/imagery/?lat=41.386792&lon=2.163628&date=2015-02-01&dim=0.3&cloud_score=True&api_key=XXX\"\n",
    "response = urllib.request.urlopen(url)\n",
    "response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the answer in a better format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "url = \"https://api.nasa.gov/planetary/earth/imagery/?lat=41.386792&lon=2.163628&date=2015-02-01&dim=0.3&cloud_score=True&api_key=XXXXX\"\n",
    "response = urllib.request.urlopen(url)\n",
    "json_response = json.loads(response.read())\n",
    "json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('scraped_image.bmp','wb')\n",
    "data = urllib.request.urlopen(json_response['url']).read()\n",
    "f.write(data)\n",
    "f.close()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "im=plt.imread('scraped_image.bmp')\n",
    "plt.imshow(im,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, lets go for a more programatic way of doing this stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "earth_url = 'https://api.nasa.gov/planetary/earth/imagery'\n",
    "\n",
    "def get_earth_photo(lon, lat, dim=0.1, date = '2015-6-6', api_key='DEMO_KEY'):\n",
    "    params = { 'lon': lon, 'lat':lat, 'dim':dim, 'api_key': api_key }\n",
    "    str_params = \"/?lat=\"+str(params['lat'])+\"&lon=\"+str(params['lon'])+\"&dim=\"+str(params['dim'])+\"&date=\"+date+\"&api_key=\"+params['api_key']\n",
    "    \n",
    "    response = urllib.request.urlopen(earth_url+str_params)\n",
    "    json_response = json.loads(response.read())\n",
    "    return json_response['url']\n",
    "\n",
    "##### CHANGE api_key with correct ID #############\n",
    "print(get_earth_photo(2.163628,41.386792,api_key=\"XXXXXX\"))\n",
    "\n",
    "f = open('scraped_image.bmp','wb')\n",
    "data = urllib.request.urlopen(get_earth_photo(2.163628,41.386792,dim=0.3,date=\"2015-02-01\",api_key=\"XXXXX\")).read()\n",
    "f.write(data)\n",
    "f.close()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "im=plt.imread('scraped_image.bmp')\n",
    "plt.imshow(im,interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATUS VALUES:\n",
    "\n",
    "+ 200 — everything went okay, and the result has been returned (if any)\n",
    "+ 301 — the server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint name is changed.\n",
    "+ 400 — the server thinks you made a bad request. This can happen when you don’t send along the right data, among other things.\n",
    "+ 401 — the server thinks you’re not authenticated. This happens when you don’t send the right credentials to access an API (we’ll talk about authentication in a later post).\n",
    "+ 403 — the resource you’re trying to access is forbidden — you don’t have the right permissions to see it.\n",
    "+ 404 — the resource you tried to access wasn’t found on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS MATERIAL: Scrapping twitter data with an API wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard way for programatically communicating with a web service is using the API (Application Programing Interface) whenever it is provided. Twitter provides several APIs. The two most important ones are the RESTful API for static queries (e.g. user's friends and followers, check timelines, etc) and the Streaming API for retrieving live data. The REST API identifies Twitter applications and users using OAuth; responses are available in JSON. The Streaming API should not need authentication.\n",
    "\n",
    "Ex. \n",
    "\n",
    "https://api.twitter.com/oauth/authenticate?oauth_token=XXXXXXXXXXXXXX\n",
    "\n",
    "https://api.twitter.com/1.1/followers/ids.json?cursor=-1&screen_name=my_user_name&count=5000\n",
    "\n",
    "Building these queries is not always easy, thus we may use a wrapper around the API. This is what **tweepy** does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the API with authentification (needed for the RESTful API)\n",
    "\n",
    "From wikipedia:\n",
    "\n",
    ">\"Web service APIs that adhere to the architectural constraints are called RESTful. HTTP based RESTful APIs are defined with these aspects:\n",
    "\n",
    "> <ul><li>base URI (Uniform Resource Identifier), such as http://example.com/resources/\n",
    "<li>an Internet media type for the data. This is often JSON but can be any other valid Internet media type (e.g. XML, Atom, microformats, images, etc.)</li>\n",
    "<li>standard HTTP methods (e.g., GET [retrieve], PUT[idempotent update/create], POST[update/create], or DELETE)</li>\n",
    "<li>hypertext links to reference state</li>\n",
    "<li>hypertext links to reference related resources\"</li>\n",
    "</ul>\n",
    "\n",
    "If we want to use the RESTful API in Twitter we have to follow these steps:\n",
    "<ul>\n",
    "<li>From your twitter account we want to generate a token: https://apps.twitter.com</li>\n",
    "<li>Create a new App. This will create the API keys (consumer keys)</li>\n",
    "<li>Go to API Keys and generate a token. (access keys)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo\n",
    "import tweepy\n",
    "\n",
    "consumer_key = \"XXXX\"\n",
    "consumer_secret = \"XXXX\"\n",
    "\n",
    "access_key = \"XXXX\"\n",
    "access_secret = \"XXXXX\"\n",
    "\n",
    "#Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Do something\n",
    "USER_NAME = \"espavilat\"\n",
    "user = api.get_user(id=USER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access some basic information about the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user._json['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.friends_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.followers_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">JSON (JavaScript Object Notation), is an open standard format that uses human-readable text to transmit data objects consisting of attribute–value pairs. It is used primarily to transmit data between a server and web application, as an alternative to XML. JSON is a way to encode complicated information in a platform-independent way.  It could be considered the lingua franca of information exchange on the Internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can access the full JSON\n",
    "user._json['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access all the information as it was a dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juser = user._json\n",
    "print (juser['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply our basic scrape knowledge and use urllib2 to retrieve more interesting infomation, such as the profile image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = juser['profile_image_url']\n",
    "print (img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "f = open('scraped_image.bmp','wb')\n",
    "im_str=urlopen(img_url).read()\n",
    "f.write(im_str)\n",
    "f.close()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "im=plt.imread('scraped_image.bmp')\n",
    "plt.imshow(im,interpolation='nearest')\n",
    "plt.title(juser['screen_name'],size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to retrieve the list of follower ids. There are two ways for doing so. Both uses the `api.followers_ids` function. The function returns a maximum of 100 ids. If we want to get all of them we may use a pagination variable `cursor`. This can be managed directly in the call `api.followers_ids(id, cursor)` or using a `Cursor` object with the `pages` method that handles the cursor implicitly. This second method is illustrated in the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving all the followers\n",
    "import time\n",
    "ids = []\n",
    "for page in tweepy.Cursor(api.followers_ids, screen_name=USER_NAME).pages():\n",
    "    ids.extend(page)\n",
    "    time.sleep(60)  #This should be 60 to avoid limit rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `sleep` command. This is needed to respect the hourly limit rates of the Twitter API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#friends (screen_name) or follower_ids\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document={}\n",
    "document['user'] = user.id\n",
    "document['followers'] = ids[:]\n",
    "\n",
    "# Create the connection to MongoDB\n",
    "try:\n",
    "    conn=pymongo.MongoClient()\n",
    "    print (\"Connection to Mongo Daemon successful!!!\")\n",
    "except pymongo.errors.ConnectionFailure as e:\n",
    "    print (\"Could not connect to MongoDB: %s\" % e )\n",
    "db = conn['twitter']\n",
    "collection = db['twitter_users']\n",
    "collection.insert_one(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in collection.find():\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['followers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-error\" style = \"border-radius:10px;border-width:3px;border-color:darkred;font-family:Verdana,sans-serif;font-size:16px;\"> **TAKE HOME EXERCISE:** Given a starting user ID, retrieve the user ids corresponding to the set of followers up to two depth levels. This is the followers of the followers of the named user. This information creates a network of influence that will be used in upcoming sessions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Streaming API** works by making a request for a specific type of data — filtered by keyword, user, geographic area, or a random sample — and then keeping the connection open as long as there are no errors in the connection. The data you get back will be encoded in JSON. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main usage cases of tweepy is monitoring for tweets and doing actions when some event happens. Key component of that is the StreamListener object, which monitors tweets in real time and catches them.\n",
    "\n",
    "If we check the official twitter streaming API we see that we have several modifiers for filtering the stream, i.e. track (filter by keyword), locations (filter by geographic location), etc\n",
    "\n",
    "StreamListener has several methods, with on_data() and on_status() being the most useful ones. Here is a sample program which implements this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream,StreamListener\n",
    "\n",
    "class listener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        #Beauty print data\n",
    "        parsed = json.loads(data)\n",
    "        print (json.dumps(parsed, indent=4, sort_keys=True))\n",
    "        return True\n",
    "    def on_error(self, status):\n",
    "        print ('ERROR')\n",
    "        print (status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the twitter data filtered by location inside the following bounding box. (http://boundingbox.klokantech.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style = \"border-radius:10px;\" src=\"./files/ub_location.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterStream = Stream(auth, listener()) \n",
    "twitterStream.filter(locations=[2.1622322352,41.385987385,2.1651827408,41.3877173586])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other examples\n",
    "twitterStream = Stream(auth, listener()) \n",
    "#twitterStream.filter(track=[\"datascience\"])\n",
    "#Use http://boundingbox.klokantech.com to get the Barcelona bounding box\n",
    "twitterStream.filter(locations=[2.0504377635,41.2787636541,2.3045074059,41.4725622346])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream,StreamListener\n",
    "\n",
    "class listener(StreamListener):\n",
    "    def on_data(self, status):\n",
    "        json_data=json.loads(status)\n",
    "        print (str(json_data[\"user\"][\"screen_name\"])+' : ' + json_data[\"text\"])\n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print ('Error')\n",
    "        print (status)\n",
    "        \n",
    "# Catch all tweets in Barcelona area and print them\n",
    "twitterStream = Stream(auth, listener()) \n",
    "#twitterStream.filter(locations=[2.1622322352,41.385987385,2.1651827408,41.3877173586])\n",
    "twitterStream.filter(locations=[2.0504377635,41.2787636541,2.3045074059,41.4725622346])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fill the class in order to capture and store the data in a MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream,StreamListener\n",
    "\n",
    "class listener(StreamListener):\n",
    "    def __init__(self):\n",
    "        super(StreamListener, self).__init__()\n",
    "        try:\n",
    "            self.conn=pymongo.MongoClient()\n",
    "            print (\"Connection to Mongo Daemon successful!!!\")\n",
    "        except pymongo.errors.ConnectionFailure as e:\n",
    "            print (\"Could not connect to MongoDB: %s\" % e )\n",
    "        self.db = conn['twitter_stream']\n",
    "        self.collection = db['tweets']\n",
    "    \n",
    "    def on_data(self, status):\n",
    "        jdata = json.loads(status)\n",
    "        if 'android' in jdata[\"source\"]:\n",
    "            device = \"android\"\n",
    "        else:\n",
    "            device = \"apple\"\n",
    "        document={'text':jdata[\"text\"], 'created':jdata[\"created_at\"], 'screen_name':jdata[\"user\"][\"screen_name\"], 'device':device}        \n",
    "        self.collection.insert(document) \n",
    "        print (document)\n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print ('ERROR')\n",
    "        print (status)\n",
    "\n",
    "# Catch all tweets in Barcelona area and print them\n",
    "twitterStream = Stream(auth, listener()) \n",
    "twitterStream.filter(locations=[2.0504377635,41.2787636541,2.3045074059,41.4725622346])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check captured data\n",
    "try:\n",
    "    conn=pymongo.MongoClient()\n",
    "    print (\"Connection to Mongo Daemon successful!!!\")\n",
    "except pymongo.errors.ConnectionFailure as e:\n",
    "    print (\"Could not connect to MongoDB: %s\" % e )\n",
    "\n",
    "db = conn['twitter_stream']\n",
    "collection = db['tweets']\n",
    "collection.count()\n",
    "for doc in coll.find():\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.database_names()\n",
    "db = conn['twitter']\n",
    "coll = db.tweets\n",
    "for item in coll.find():\n",
    "    print (item['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs are nice. Most large web site provide useful APIs, e.g. Google, OpenStreetMap, Facebook, etc, subject to some use terms. However most of the web sited do not provide any kind of access to data. What to do then?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
