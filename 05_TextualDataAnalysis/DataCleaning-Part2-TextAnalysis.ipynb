{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cleaning Text\n",
    "Now we are able to process data to analyze text, numbers and symbols for particular regular expressions that can be useful for data cleaning, representation, and decision making analyses. Let's start performing some data cleaning filtering\n",
    "\n",
    "For this purpose we will first use the NLTK library characteristics (Natural Language Toolkit).\n",
    "\n",
    "<code>\n",
    "    Regular expressions and examples\n",
    "    Data cleaning:\n",
    "        Tokenining\n",
    "        Removing punctuation\n",
    "        Stemming and Lemmatizing\n",
    "        Removing tags\n",
    "    Text representation\n",
    "            TF-IDF: Term frequencies (counter)\n",
    "            Vector normalization\n",
    "            Feature weighting (Inverse Document Frequency)\\t\n",
    "            Sklearn implementation\\n\n",
    "    Learning text representations\\n\n",
    "            Stopwords\\n\n",
    "            Bag of Words\\n\n",
    "            n-grams\\n\n",
    "            Training a (naive Bayes) Classifier with NLTK: film critiques example\\n\n",
    "            Training a (naive Bayes) Classifier with TextBlob: A Tweet Sentiment Analyzer\\n\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizing text into bags of words\n",
    "\n",
    "NLTK makes it easy to convert documents-as-strings into word-vectors, a process called tokenizing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print (tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Removing punctuation\n",
    "\n",
    "Punctuation can help with tokenizers, but once you've done that, there's no reason to keep it around. There are tons of ways to remove punctuation.\n",
    "\n",
    "Let's review some useful functions\n",
    "\n",
    "re.escape: Return string with all non-alphanumerics backslashed; this is useful if you want to match an arbitrary literal string that may have regular expression metacharacters in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "#nltk.download('punkt')\n",
    "\n",
    "print(string.punctuation)\n",
    "print (re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"tokenized_docs:\\n\", tokenized_docs,'\\n')    \n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "tokenized_docs_no_punctuation = []\n",
    "for sentence in tokenized_docs:\n",
    "    new_sentence = []\n",
    "    for token in sentence: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_sentence.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_sentence)\n",
    "    \n",
    "print (\"tokenized_docs without punctuation symbols:\\n\",tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation symbols are removed, and those words containing a punctuation symbol are keeped and marked with an initial 'u'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming and Lemmatizing\n",
    "\n",
    "If you have taken linguistics, you may be familiar with morphology. This is the belief that words have a root form. If you want to get to the basic term meaning of the word, you can try applying a stemmer or lemmatizer. Here are three very popular methods ready to go right out of the NLTK box.\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1539984207/stemminglemmatization_n8bmou.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter   = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet  = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        #final_doc.append(porter.stem(word))\n",
    "        final_doc.append(snowball.stem(word)) # requires 'corpora/wordnet' -> nltk.download()\n",
    "        #final_doc.append(wordnet.lemmatize(word)) # requires 'corpora/wordnet' -> nltk.download()\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "    \n",
    "print(\"BEFORE:\\n \", tokenized_docs_no_punctuation)\n",
    "print(\"\\nAfter Stemming and Lemmatizing:\\n \", preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Removing HTML entities and tags\n",
    "We have it already implemented in NLTK!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "    \"\"\"\n",
    "    Copied from NLTK package.\n",
    "    Remove HTML markup from the given string.\n",
    "\n",
    "    :param html: the HTML string to be cleaned\n",
    "    :type html: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # First we remove inline JavaScript/CSS:\n",
    "    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n",
    "    \n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n",
    "    \n",
    "    # Next we can remove the remaining tags:\n",
    "    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
    "    \n",
    "    # Finally, we deal with whitespace\n",
    "    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "test_string  =\"<p>While many of the stories tugged <a> at the heartstrings, I never felt manipulated by the authors.\"\n",
    "test_string +=\" (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot;\"\n",
    "test_string +=\"series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\"\n",
    "\n",
    "print (test_string)\n",
    "print(\"\\n--- Cleaned text:---\" )\n",
    "print(clean_html(test_string))\n",
    "\n",
    "#Use from bs4 import BeautifulSoup  : for improved versions of tag cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Vector Space Model of text: TF-IDF (Term Frequency - Inverse Distance Frequency)\n",
    "\n",
    "Once text is analyzed based on regular expressions and cleaned by filtering using some of the previous tools, we can proceed to represent it in order to perform posterior analyses.\n",
    "\n",
    "We need to start thinking about how to translate collections of texts into quantifiable phenomena.  The easiest way to start is to think about word frequencies.\n",
    "\n",
    "Example with histograms of visual words representation:\n",
    "<img src=\"https://github.com/DataScienceUB/Postgrau/raw/d4ef21ee7506dd2725b7fd9d9607e931b1834cb5/05_TextualDataAnalysis/VisualWords.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic term frequencies\n",
    "First, let's review how to get a count of terms per document: a term frequency vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mydoclist = ['Julie loves me more than Linda loves me',\n",
    "'Jane likes me more than Julie loves me',\n",
    "'He likes basketball more than baseball']\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] +=1\n",
    "    print (tf.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's call this a first stab at representing documents quantitatively, just by their word counts (also thinking that we may have previously filtered and cleaned the text using previous approaches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lexicon(corpus): # define a set with all possible words included in all the sentences or \"corpus\"\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "\n",
    "\n",
    "\n",
    "vocabulary = build_lexicon(mydoclist)\n",
    "\n",
    "doc_term_matrix = []\n",
    "print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']\\n')\n",
    "\n",
    "for doc in mydoclist:    \n",
    "    print ('The doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    \n",
    "    print ('The tf vector for Document %d is [%s]|\\n' % ((mydoclist.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "\n",
    "print ('\\nAll combined, here is our master document term matrix: ')\n",
    "print (doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that seems reasonable enough. If any of you have any experience with machine learning, what you've just seen is the creation of a feature space. Now every document is in the same feature space, meaning that we can represent the entire corpus in the same dimensional space without having lost too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalizing vectors to L2 Norm\n",
    "Once you've got your data in the same feature space, you can start applying some machine learning methods; classifying, clustering, and so on. But actually, we've got a few problems. Words aren't all equally informative.\n",
    "\n",
    "If words appear too frequently in a single document, they're going to muck up our analysis. We want to perform some scaling of each of these term frequency vectors into something a bit more representative. In other words, we need to do some vector normalizing.\n",
    "\n",
    "One possibility is to ensure that the L2 norm of each vector is equal to 1. Here's some code that shows how this is done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "\n",
    "print ('A regular old document term matrix: ') \n",
    "print (np.matrix(doc_term_matrix))\n",
    "print ('\\nA document term matrix with row-wise L2 norms of 1:')\n",
    "print (np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see immediately that we've scaled down vectors such that each element is between [0, 1], without losing too much valuable information.\n",
    "\n",
    "Why would we care about this kind of normalizing? Think about it this way; if you wanted to make a document seem more related to a particular topic than it actually was, you might try boosting the likelihood of its inclusion into a topic by repeating the same word over and over and over again. Frankly, at a certain point, we're getting a diminishing return on the informative value of the word. So we need to scale down words that appear too frequently in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### IDF frequency weighting\n",
    "But we're still not there yet. Just as all words aren't equally valuable within a document, **not all words are valuable across all documents**. We can try reweighting every word by its inverse document frequency. Let's see what's involved in that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    '''Num documents containing the word'''\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount +=1\n",
    "    return doccount \n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist) #numer of documents\n",
    "    df = numDocsContaining(word, doclist) \n",
    "    return np.log(n_samples / (float(df)) )\n",
    "\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "\n",
    "print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']\\n')\n",
    "print ('The inverse document frequency (IDF) vector is\\n [' \n",
    "       + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have a general sense of information values per term in our vocabulary, accounting for their relative frequency across the entire corpus. Recall that this is an inverse! The lower the value, the more frequent it is.\n",
    "\n",
    "To get TF-IDF weighted word vectors, we have to perform the simple calculation of **tf * idf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "print (my_idf_matrix)\n",
    "print(my_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have converted our IDF vector into a matrix of size BxB, where the diagonal is the IDF vector. That means we can perform now multiply every term frequency vector by the inverse document frequency matrix. Then to make sure we are also accounting for words that appear too frequently within documents, we'll normalize each document using L2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_tfidf = []\n",
    "\n",
    "#performing tf-idf matrix multiplication\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "\n",
    "#normalizing\n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tfidf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tfidf_vector))\n",
    "                                    \n",
    "print (vocabulary)\n",
    "print (np.matrix(doc_term_matrix_tfidf_l2)) # np.matrix() just to make it easier to look at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's see an efficient implementation of the previous approach using scikits-learn, which ensures that you don't have to worry about the efficiency of all the previous steps.\n",
    "\n",
    "**NOTE**: The values you get from the TfidfVectorizer/TfidfTransformer will be different than what we have computed by hand. This is because scikits-learn uses an adapted version of Tfidf to deal with divide-by-zero errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
    "print (\"Vocabulary:\", count_vectorizer.vocabulary_,'\\n')\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix)\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "\n",
    "print(mydoclist)\n",
    "print (tf_idf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf.idf_, index=count_vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or more directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "\n",
    "print (tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can fit new observations into this vocabulary space like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\n",
    "new_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\n",
    "print (tfidf_vectorizer.vocabulary_)\n",
    "print (new_term_freq_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we didn't get words like 'watches' in the new_term_freq_matrix. That's because we trained the object on the documents in mydoclist, and that word doesn't appear in the vocabulary from that corpus. In other words, it's out of the lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning text representations\n",
    "When using NLTK classifier, it is noted that the classifer expects dict style feature sets, so we need to transform the text into a dict.\n",
    "We can use previous TF-IDF. Let's see another implementation: Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, write a Feature extractor (the following is taken from nltk-trainer package)\n",
    "\n",
    "# download featx.py (written by Perkins)\n",
    "\n",
    "from nltk import probability\n",
    "\n",
    "def bag_of_words(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "def bag_of_words_in_set(words, wordset):\n",
    "        return bag_of_words(set(words) & wordset)\n",
    "    \n",
    "def word_counts(words):\n",
    "        return dict(probability.FreqDist((w for w in words)))\n",
    "\n",
    "def word_counts_in_set(words, wordset):\n",
    "        return word_counts((w for w in words if w in wordset))\n",
    "\n",
    "def train_test_feats(label, instances, featx=bag_of_words, fraction=0.75):\n",
    "        labeled_instances = [(featx(i), label) for i in instances]\n",
    "        \n",
    "        if fraction != 1.0:\n",
    "                l = len(instances)\n",
    "                cutoff = int(math.ceil(l * fraction))\n",
    "                return labeled_instances[:cutoff], labeled_instances[cutoff:]\n",
    "        else:\n",
    "                return labeled_instances, labeled_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bag_of_words(['this', 'is', 'awesome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "bag_of_words_not_in_set(['this','is','awesome'],['this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile = 'english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "bag_of_non_stopwords(['this','is','awesome'])\n",
    "\n",
    "#nltk.download()   # try to use this is you need some package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()   # try to use this is you need some package\n",
    "from nltk.corpus import stopwords\n",
    "print (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a (naive Bayes) Classifier with TextBlob: A Tweet Sentiment Analyzer\n",
    "\n",
    "\n",
    "We will train a simple sentiment analyzer trained on a small dataset of fake tweets. To begin, we’ll import the text.classifiers and create some training and test data.\n",
    "\n",
    "https://blog.twitter.com/2012/a-new-barometer-for-the-election\n",
    "\n",
    "\"( Wednesday, August 1, 2012 | By Adam Sharp (@AdamS) [16:00 UTC] ) One glance at the numbers, and it’s easy to see why pundits are already calling 2012 “the Twitter election.” More Tweets are sent every two days today than had ever been sent prior to Election Day 2008 — and Election Day 2008’s Tweet volume represents only about six minutes of Tweets today.\"\n",
    "\n",
    "\"Each day, the Index evaluates and weighs the sentiment of Tweets mentioning Obama or Romney relative to the more than 400 million Tweets sent on all other topics. For example, a score of 73 for a candidate indicates that Tweets containing their name or account name are on average more positive than 73 percent of all Tweets.\"\n",
    "\n",
    "<img src=\"Tweetpolitics.png\\\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U textblob\n",
    "#!conda install -c https://conda.anaconda.org/sloria textblob -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    (\"What an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new classifier by passing training data into the constructor for a NaiveBayesClassifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.show_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now classify arbitrary text using the NaiveBayesClassifier.classify(text) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.classify(\"Their burgers are amazing\")  # \"pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.classify(\"I don't like their pizza.\")  # \"neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another way to classify strings of text is to use TextBlob objects. You can pass classifiers into the constructor of a TextBlob.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "blob = textblob.TextBlob(\"The beer was amazing. \"\n",
    "                \"But the hangover was horrible. My boss was not happy.\", classifier=cl)\n",
    "print (blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then call the classify() method on the blob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.classify()  # \"neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also take advantage of TextBlob’s sentence tokenization and classify each sentence indvidually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print(sentence)\n",
    "    print(sentence.classify())\n",
    "# \"pos\", \"neg\", \"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.accuracy(test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our classifier by adding more training and test data. Here we’ll add data from the movie review corpus which was downloaded with NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(reviews)\n",
    "\n",
    "new_train, new_test = reviews[0:300], reviews[301:400]\n",
    "\n",
    "print('Review:' )\n",
    "print(' '.join(new_train[0][0]))\n",
    "print(\"\\nSentiment: \"+ new_train[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s see what one of these documents looks like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now update our classifier with the new training data using the update(new_data) method, as well as test it using the larger test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cl.update(new_train) # it takes a while\n",
    "accuracy = cl.accuracy(new_test) \n",
    "print(\"Accuracy: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here’s the full, updated script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    (\"What an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]\n",
    "\n",
    "processed_features = []\n",
    "def clean_text(data):\n",
    "    for sentence in range(0, len(data)):  \n",
    "        # Remove all the special characters\n",
    "        processed_feature = re.sub(r'\\W', ' ', str(data[sentence][0]))\n",
    "\n",
    "        # remove all single characters\n",
    "        processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        processed_feature = processed_feature.lower()\n",
    "\n",
    "        data[sentence] = (processed_feature,data[sentence][1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_clean = clean_text(train)\n",
    "test_clean = clean_text(test)\n",
    "\n",
    "cl = NaiveBayesClassifier(train_clean)\n",
    "accuracy = cl.accuracy(test_clean)\n",
    "print(\"Accuracy: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Removing HTML entities and tags\n",
    "We have it already implemented in NLTK!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string =\"<p>While many of the stories tugged <a> at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\"\n",
    "print(\"ORIGINAL TEXT:\")\n",
    "print (test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def clean_html(html):\n",
    "    \"\"\"\n",
    "    Copied from NLTK package.\n",
    "    Remove HTML markup from the given string.\n",
    "\n",
    "    :param html: the HTML string to be cleaned\n",
    "    :type html: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # First we remove inline JavaScript/CSS:\n",
    "    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n",
    "    # Next we can remove the remaining tags:\n",
    "    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
    "    # Finally, we deal with whitespace\n",
    "    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "print(\"\\nCLEANED TEXT:\")\n",
    "print(clean_html(test_string) )\n",
    "\n",
    "#Use from bs4 import BeautifulSoup  : for improved versions of tag cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read data from moneycontrol\n",
    "Let's get something more complex!! An html page from moneycontrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "### Lets get a document from moneycontrol\n",
    "#Get each article from site\n",
    "url = 'https://www.moneycontrol.com/news/business/earnings/polycab-india-q4-net-profit-down-27-3-to-spend-rs-175-200cr-annually-on-capex-3972161.html'\n",
    "results = requests.get(url)\n",
    "print(results.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After cheking this website we have seen that the new is located within the  \n",
    "<div class=\"arti-flow\" id=\"article-main\"> </code></text>-->\n",
    "\n",
    "So let's get this text using the **BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def clean_html_text(text):\n",
    "    #Extract text\n",
    "    results_text = BeautifulSoup(text)\n",
    "    for s in results_text(['script', 'style']):\n",
    "            s.decompose()\n",
    "    # look for the area where the new is located\n",
    "    extract_text = results_text.find(class_='arti-flow')\n",
    "    # clean the text\n",
    "    clean_text = re.sub(r'<[^>]*?>', '', extract_text.text)\n",
    "    # tags since comments can contain '>' characters.\n",
    "    clean_text = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", clean_text)\n",
    "    clean_text = re.sub(r\"\\n\", \"\", clean_text)\n",
    "    return clean_text\n",
    "\n",
    "raw_doc = clean_html_text(results.text)\n",
    "print(\"\\n------------- raw doc -------------\")\n",
    "print(raw_doc)\n",
    "\n",
    "tokenized_doc = word_tokenize(raw_doc) \n",
    "tokenized_doc = [regex.sub(u'', token)  for token in tokenized_doc if not regex.sub(u'', token) == u'']\n",
    "\n",
    "print(\"\\n------------- tokenized_doc -------------\")\n",
    "print (tokenized_doc)\n",
    "\n",
    "final_doc = []\n",
    "final_doc_stem = []\n",
    "for word in tokenized_doc:\n",
    "    final_doc.append(word)\n",
    "    final_doc_stem.append(snowball.stem(word))\n",
    "print(\"\\n------------- final_doc_stem -------------\")\n",
    "\n",
    "print(final_doc_stem)\n",
    "print(\"\\n------------- bag_of_words -------------\")\n",
    "\n",
    "print(bag_of_words(final_doc_stem).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Lets get a document from moneycontrol\n",
    "#Get each article from site\n",
    "urls = []\n",
    "urls.append('https://www.moneycontrol.com/news/business/kesoram-back-in-black-with-net-profit-of-rs-20-7-cr-in-q4fy19-3978841.html')\n",
    "urls.append('https://www.moneycontrol.com/news/business/kpit-net-profit-at-rs-30-9-cr-in-march-quarter-3983141.html')\n",
    "urls.append('https://www.moneycontrol.com/news/business/earnings/minda-ind-consolidated-march-2019-net-sales-at-rs-1486-46-crore-up-8-43-y-o-y-3982921.html')\n",
    "urls.append('https://www.moneycontrol.com/news/business/bajaj-finserv-q4-net-profit-up-32-at-rs-839-cr-3984091.html')\n",
    "urls.append('https://www.moneycontrol.com/news/business/earnings/bajaj-allianz-general-insurance-q4-net-profit-down-55-6-yoy-at-rs-83-crore-3983751.html')\n",
    "urls.append('https://www.moneycontrol.com/news/business/earnings/gabriel-india-standalone-march-2019-net-sales-at-rs-510-31-crore-up-2-73-y-o-y-3982861.html')\n",
    "urls.append('https://www.moneycontrol.com/news/business/earnings/savani-financia-standalone-march-2019-net-sales-at-rs-0-03-crore-down-4-99-y-o-y-3982761.html')\n",
    "\n",
    "mydoclist = []\n",
    "for url in urls:\n",
    "    results = requests.get(url)\n",
    "    raw_doc = clean_html_text(results.text)\n",
    "    raw_doc = regex.sub(u'', raw_doc) \n",
    "    raw_doc = re.sub(r\"\\d\", \"\", raw_doc)\n",
    "\n",
    "    tokenized_doc = word_tokenize(raw_doc) \n",
    "    #tokenized_doc = [regex.sub(u'', token)  for token in tokenized_doc if not regex.sub(u'', token) == u'']\n",
    "    \n",
    "    final_doc = []\n",
    "    final_doc_stem = []\n",
    "    for word in tokenized_doc:\n",
    "        final_doc.append(word)\n",
    "        final_doc_stem.append(snowball.stem(word))\n",
    "    \n",
    "    mydoclist.append(' '.join(final_doc_stem))\n",
    "    #print(\"Stemmming  data:\\n\", final_doc_stem)\n",
    "mydoclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 2,max_df =4)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 2,max_df = 4,ngram_range=(1,3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print (tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Or we can get existing datasets with positive/negative sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df_dict = pd.read_csv('dict.csv')\n",
    "vocabulary = df_dict['okay from USFDA'].values\n",
    "print ('Our vocabulary vector is:\\n [' + ', '.join(list(vocabulary)) + ']')\n",
    "\n",
    "vocabulary_stem = list(set([ snowball.stem(word) for word in vocabulary]))\n",
    "print(vocabulary_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=vocabulary_stem)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spell and Grammar Correction ??\n",
    "\n",
    "Depending on the problem, it might be interesting to perform spelling and grammar correction before analysing the content.\n",
    "\n",
    "Data cleansing is all about getting rid of the “noise” in the data. But is your decision, and always depend on the applications, to decide what content within the data is noise, and what is not noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Summary of some useful functions\n",
    "\n",
    "<img src=\"summary.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
