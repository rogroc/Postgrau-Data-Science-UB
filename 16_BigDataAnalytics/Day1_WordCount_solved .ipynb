{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n# Contando palabras: Construye una aplicacion que cuente palabras de forma eficiente\n\nEste laboratorio usara las tecnologias descritas en los materiales del curso sobre Spark para desarrollar una aplicacion de conteo de palabras. \n\nCon el uso masivo de Internet y las redes sociales, el volumen de texto no estructurado esta creciendo dramaticamente, y Spark es una gran herramienta para analizar este tipo de datos. En esta PEC, vamos a escribir codigo para encontrar las palabras mas comunes en las [obras completas de William Shakespeare](http://www.gutenberg.org/ebooks/100) recuperados a partir de [Proyecto Gutenberg](http://www.gutenberg.org/wiki/Main_Page).\n\nLo mas interesante de la forma de trabajar en esta practica es que podria escalarse para, por ejemplo, encontrar las palabras mas comunes en Wikipedia.\n\n## Durante esta PEC vamos a cubrir:\n\n* *Parte 1:* Creacion de un RDD y un pair RDD\n* *Parte 2:* Contar palabras usando un pair RDD\n* *Parte 3:* Encontrar las palabras individuales y su frecuencia de aparicion media\n* *Parte 4:* Aplicar las funcionalidades desarrolladas a un archivo de texto* \n* *Parte 5:* Calcular algunos estadisticos*\n\n\n> Como referencia a todos los detalles de los metodos que se usan en esta practica usar:\n> * [API Python de Spark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"],"metadata":{}},{"cell_type":"markdown","source":["## Parte 1: Creacion de un RDD y un pair RDDs\n\nEn esta seccion, exploraremos como como crear RRDs usando `parallelize` y como aplicar pair RDDs al problema del conteo de palabras.\n\n### (1a) Creacion de un RDD\nEmpezemos generando un RDD a partir de una lista de Python y el metodo `sc.parallelize`. Luego mostraremos por pantalla el tipo de la variable generada."],"metadata":{}},{"cell_type":"code","source":["wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\nwordsRDD = sc.parallelize(wordsList, 4)\n# Print out the type of wordsRDD\nprint type(wordsRDD)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### (1b) Crear el plural de las palabas y testear\n\nVamos a utilizar una transformacion `map()` para incorporar la letra 's' a cada uno de los strings almacenados en el RDD que acabamos de crear. Vamos a definir una funcion de Python que devuelva una palabra, que se le ha pasado como parametro, incorporando una \"s\" al final de la misma. Reemplazar el texto `<FILL IN>` con la solucion propuesta. Despues de haber definido correctamente la funcion `makePlural`, ejecutar la segunda celda que contiene un assert de test. Si la solucion es correcta, se imprimira `1 test passed`.\n\nEsta sera la forma habitual de trabajar en las PECs. Los ejercicios contendran una explicacion de lo que se espera, seguido de una celda de codigo con uno o mas `<FILL IN>`. Las celdas que necesiten ser modificadas contendran el texto `# TODO: Replace <FILL IN> with appropriate code` en la primera linea.\n\nUna vez se hayan sustituido todos los `<FILL IN>` por el codigo Python adecuado, ejecutar la celda, y posteriormente ejecutar la celda siguiente de test para comprobar que que la solucion es la esperada."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndef makePlural(word):\n    \"\"\"Adds an 's' to `word`.\n\n    Note:\n        This is a simple function that only adds an 's'.  \n\n    Args:\n        word (str): A string.\n\n    Returns:\n        str: A string with 's' added to it.\n    \"\"\"\n    return word + 's'\n\nprint makePlural('cat')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Load in the testing code and check to see if your answer is correct\n# If incorrect it will report back '1 test failed' for each failed test\n# Make sure to rerun any cell you change before trying the test again\nfrom databricks_test_helper import Test\n# TEST Pluralize and test (1b)\nTest.assertEquals(makePlural('rat'), 'rats', 'incorrect result: makePlural does not add an s')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### (1c) Aplicar `makePlural` a nuestro RDD\n\nAhora es el momento de aplicar nuestra funcion `makePlural()` a todos los elementos del RDD usando una transformacion [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map). Posteriormente ejecutar la accion [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) para obtener el RDD transformado."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\npluralRDD = wordsRDD.map(lambda w: makePlural(w))\nprint pluralRDD.collect()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# TEST Apply makePlural to the base RDD(1c)\nTest.assertEquals(pluralRDD.collect(), ['cats', 'elephants', 'rats', 'rats', 'cats'],\n                  'incorrect values for pluralRDD')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### (1d) Ejecutar una funcion `lambda` en un `map`\n\nVamos a crear el mismo RDD usando una `lambda` function en lugar de una funcion con nombre."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\npluralLambdaRDD = wordsRDD.map(lambda w : w + 's')\nprint pluralLambdaRDD.collect()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# TEST Pass a lambda function to map (1d)\nTest.assertEquals(pluralLambdaRDD.collect(), ['cats', 'elephants', 'rats', 'rats', 'cats'],\n                  'incorrect values for pluralLambdaRDD (1d)')"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### (1e) Numero de caracteres de cada una de las palabras\n\nAhora vamos a usar un `map()` y una funcion lambda `lambda` para obtener el numero de caracteres de cada palabra. Usaremos `collect` para guardar este resultado directamente en una variable."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\npluralLengths = (pluralRDD\n                 .map(lambda a: len(a))\n                 .collect())\nprint pluralLengths"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# TEST Length of each word (1e)\nTest.assertEquals(pluralLengths, [4, 9, 4, 4, 4],\n                  'incorrect values for pluralLengths')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### (1f) Pair RDDs\n\nEl siguiente paso para completar nuestro programa de conteo de palabras en crear un nuevo tipo de RDD, llamado pair RDD. Un pair RDD es un RDD donde cada elemento es un tupla del estilo `(k, v)` donde `k` es la clave y `v` es su valor correspondiente. En este ejemplo, crearemos una pair RDD consistente en tuplas con el formato `('<word>', 1)` para cada elemento de nuestro RDD basico.\n\nPodemos crear nuestro pair RDD usando una transformacion `map()` con una `lambda()` function que cree un nuevo RDD."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nwordPairs = wordsRDD.map(lambda w: (w,1))\nprint wordPairs.collect()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# TEST Pair RDDs (1f)\nTest.assertEquals(wordPairs.collect(),\n                  [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)],\n                  'incorrect value for wordPairs')"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Parte 2: Contar palabras usando un pair RDD\n\nAhora, contaremos el numero de veces que una palabra en particular aparece en el RDD. Esta operacion se puede realizar de una infinidad de maneras, pero algunas seran mucho menos eficientes que otras.\n\nUn solucion muy sencilla seria usar `collect()` sobre todos los elementos devolverlos al driver y alli contarlos. Mientras esta forma de trabajar podria funcionar con textos relativamente cortos, nosotros lo que queremos es poder trabajar con textos de cualquier longitud. Adicionalmente, ejecutar todo el calculo en el driver es mucho mas lento que ejecutarlo en paralelo en los workers. Por estos motivos, en esta practica usaremos operaciones paralelizables.\n\n%md\n### (2a) Usando `groupByKey()`\nUna primera solucion a nuestro problema, luego veremos que hay otras mucho mas eficientes, se podria basar en la transformacion [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey). Como su nombre indica, la transformacion `groupByKey()` agrupa todos los elementos de un RDD que compartan la misma clave en una unica lista dentro de una de las particiones.\n\nEsta operacion plantea dos problemas:\n  + Esta operacion necesita mover todos los valores dentro de la particion adecuada. Esto satura la red. \n  + Las listas generadas pueden llegar a ser muy grandes llegando incluso a saturar la memoria de alguno de los trabajadadores\n  \nUtiliza `groupByKey()` para generar un pair RDD del tipo `('word', iterator)`."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n# Note that groupByKey requires no parameters\nwordsGrouped = wordPairs.groupByKey().mapValues(list)\nfor key, value in wordsGrouped.collect():\n    print '{0}: {1}'.format(key, list(value))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# TEST groupByKey() approach (2a)\nTest.assertEquals(sorted(wordsGrouped.mapValues(lambda x: list(x)).collect()),\n                  [('cat', [1, 1]), ('elephant', [1]), ('rat', [1, 1])],\n                  'incorrect value for wordsGrouped')"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### (2b) Utiliza `groupByKey()` para obtener los conteos\n\nUsando la transformacion `groupByKey()` crea un RDD que contenga 2 elementos, donde cada uno de ellos sea un par palabra (clave) iterador de Python (valor).\n\nLuego suma todos los valores de iterador usando una transformacion `map()`. El resultado debe ser un pair RDD que contenga las parejas (word, count)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nwordCountsGrouped = wordsGrouped.mapValues(len)\nprint wordCountsGrouped.collect()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# TEST Use groupByKey() to obtain the counts (2b)\nTest.assertEquals(sorted(wordCountsGrouped.collect()),\n                  [('cat', 2), ('elephant', 1), ('rat', 2)],\n                  'incorrect value for wordCountsGrouped')\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["** (2c) Conteo usando `reduceByKey` **\n\nUna mejor solucion es comenzar desde un pair RDD y luego usar la transformacion [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) para crear un nuevo pair RDD. La transformacion `reduceByKey()` agrupa todas las parejas que comparten la misma clave. Posteriormente aplica la funcion que se le pasa por parametro agrupando los valores de dos en dos. Este proceso se repite iterativamente hasta que obtenemos un unico valor agregado para cada una de las claves del pair RDD. `reduceByKey()` opera aplicando la funcion primero dentro de cada una de las particiones de forma independiente, y posteriormente unicamente comparte los valores agregados entre particiones diferentes, permitiendole escalar de forma eficiente ya que no tiene necesidad de desplazar por la red una gran cantidad de datos."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n# Note that reduceByKey takes in a function that accepts two values and returns a single value\nwordCounts = wordPairs.reduceByKey(lambda a,b : a+b)\nprint wordCounts.collect()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# TEST Counting using reduceByKey (2c)\nTest.assertEquals(sorted(wordCounts.collect()), [('cat', 2), ('elephant', 1), ('rat', 2)],\n                  'incorrect value for wordCounts')"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### (2d) Ahora todo junto\n\nLa version mas compleja del codigo ejecuta primero un `map()` sobre el pair RDD, la transformacion `reduceByKey()`, y finalmente la accion `collect()` en una unica linea de codigo."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nwordCountsCollected = (wordsRDD\n                       .map(lambda w: (w,1))\n                       .reduceByKey(lambda a,b : a+b)\n                       .collect())\nprint wordCountsCollected"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# TEST All together (2d)\nTest.assertEquals(sorted(wordCountsCollected), [('cat', 2), ('elephant', 1), ('rat', 2)],\n                  'incorrect value for wordCountsCollected')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Parte 3: Encontrar las palabras individuales y su frecuencia de aparicion media\n\n### (3a) Palabras unicas\n\nCalcular el numero de palabras unicas en `wordsRDD`. Puedes utitlziar otros RDDs que hayas creado en esta practica si te resulta mas sencillo."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nuniqueWords = wordCounts.count()\nprint uniqueWords"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# TEST Unique words (3a)\nTest.assertEquals(uniqueWords, 3, 'incorrect count of uniqueWords')"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### (3b) Calular la media usando `reduce()`\n\nEncuentra la frequencia media de aparicion de palabras en `wordCounts`.\n\nUtiliza la accion `reduce()` para sumar los conteos en `wordCounts` y entonces divide por el numero de palabras unicas. Para realizar esto primero aplica un `map()` al pair RDD `wordCounts`, que esta formado por tuplas con el formato (key, value), para convertirlo en un RDD de valores."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nfrom operator import add\ntotalCount = (wordCounts\n              .map(lambda e: e[1])\n              .reduce(lambda a,b: a+b))\naverage = totalCount / float(uniqueWords)\nprint totalCount\nprint round(average, 2)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# TEST Mean using reduce (3b)\nTest.assertEquals(round(average, 2), 1.67, 'incorrect value of average')"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## Parte 4: Aplicar las funcionalidades desarrolladas a un archivo de texto\n\nPara esto hemos de construir una funcion `wordCount`, capaz de trabajar con datos del mundo real que suelen presentan problemas como el uso de mayusculas o minusculas, puntuacion, acentos, etc. Posteriormente, cargar los datos de nuestra fuente de datos y finalmente, calular el conteo de palabras sobre los datos procesados.\n\n### (4a) funcion `wordCount`\n\nPrimero, define una funcion para el conteo de palabras. Deberias reusar las tecnicas que has visto en los apartados anteriores de esta practica. Dicha funcion, ha de tomar un RDD que contenga una lista de palabras, y devolver un pair RDD que contenga todas las palabras con sus correspondientes conteos."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndef wordCount(wordListRDD):\n    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n\n    Args:\n        wordListRDD (RDD of str): An RDD consisting of words.\n\n    Returns:\n        RDD of (str, int): An RDD consisting of (word, count) tuples.\n    \"\"\"\n    return wordListRDD.map(lambda w: (w,1)).reduceByKey(lambda a,b : a+b)\nprint wordCount(wordsRDD).collect()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# TEST wordCount function (4a)\nTest.assertEquals(sorted(wordCount(wordsRDD).collect()),\n                  [('cat', 2), ('elephant', 1), ('rat', 2)],\n                  'incorrect definition for wordCount function')"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["### (4b) Mayusculas y puntuacion\n\nLos ficheros del mundo real son mucho mas complejos que los que hemos estado usando en esta PAC. Algunos de los problemas que son necesarios de solucionar son:\n  + Las palabras deben de contarse independientemente de si estan en mayuscula o minuscula (por ejemplo, Spark y spark deberian contarse como la misma palabra).\n  + Todos los signos de puntuacion han de eliminarse.\n  + Cualquier espacio al principio o al final de la palabra ha de eliminarse.\n  \nDefine la funcion `removePunctuation` que convierta todo el texto a minusculas, elimine los signos de puntuacion, y elimine los espacios al principio y fin de cada palabra. Usa el modulo de Python [re](https://docs.python.org/2/library/re.html) para eliminar cualquier caracter que no sea una letra, un numero o un espacio.\n\nSino estas familiarizado con las expresiones regulares deberias revisar [este tutorial](https://developers.google.com/edu/python/regular-expressions). Alternativamente, [esta web](https://regex101.com/#python) es de gran ayuda para debugar tus expresiones regulares.\n\n**Hints**\n\n1. Usa la funcion [re.sub()](https://docs.python.org/2.7/library/re.html#re.sub).\n2. Para nuestros propositos, \"puntuacion\" significa \"no alphabetico, numerico, o espacio.\" La expresion regular que define estos caracteres es: `[^A-Za-z\\s\\d]`\n3. No usar `\\W`, ya que retendra los guiones bajos."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nimport re\ndef removePunctuation(text):\n    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n\n    Note:\n        Only whitespace, letters, and numbers should be retained.  Other characters should should be\n        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n        punctuation is removed.\n\n    Args:\n        text (str): A string.\n\n    Returns:\n        str: The cleaned up string.\n    \"\"\"\n    # define punctuation\n    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    # remove punctuation from the string\n    no_punct = \"\"\n    for char in text:\n       if char not in punctuations:\n         no_punct = no_punct + char\n    return no_punct.lower().strip()#translate(string.maketrans(\"\",\"\"), string.punctuation).strip()\nprint removePunctuation('Hi, you!')\nprint removePunctuation(' No under_score!')\nprint removePunctuation(' *      Remove punctuation then spaces  * ')"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# TEST Capitalization and punctuation (4b)\nTest.assertEquals(removePunctuation(\" The Elephant's 4 cats. \"),\n                  'the elephants 4 cats',\n                  'incorrect definition for removePunctuation function')"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["### (4c) Cargar un fichero de texto\n\nPara la siguiente parte, usaremos las [Obras completas de William Shakespeare](http://www.gutenberg.org/ebooks/100) del [Proyecto Gutenberg](http://www.gutenberg.org/wiki/Main_Page). Para convertir un fichero de texto en un RDD, usaremos el metodo `SparkContext.textFile()`. Tambien usaremos la funcion que acabamos de crear `removePunctuation()` dentro de una transformacion `map()` para eliminar todos los caracteres no alphabeticos, numericos or espacios. Dado que el fichero es bastante grandre, usaremos `take(15)`, de forma que tan solo imprimiremos por pantalla las 15 primeras lineas."],"metadata":{}},{"cell_type":"code","source":["%fs"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# Tan solo ejecuta este codigo\nimport os.path\nfileName = \"dbfs:/\" + os.path.join('databricks-datasets', 'cs100', 'lab1', 'data-001', 'shakespeare.txt')\n\nshakespeareRDD = sc.textFile(fileName, 8).map(removePunctuation)\nprint '\\n'.join(shakespeareRDD\n                .zipWithIndex()  # to (line, lineNum)\n                .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'\n                .take(15))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["### (4d) Extraer las palabras de las lineas\n\nAntes de poder usar la funcion `wordcount()`, hemos de solucionar dos problemas con el formato del RDD:\n  + El primer problema es que necesitamos dividir cada linea por sus espacios. ** Esto lo solucionaremos en el apartado (4d). **\n  + El segundo problema es que necesitamos filtar las lineas completamente vacias. ** Esto lo solucionaremos en el apartado (4e). **\n\nPara aplicar una transformacion que divida cada elemento del RDD por sus espacios, hemos de aplicar la funcion incorporada en los strings de Python [split()](https://docs.python.org/2/library/string.html#string.split). Cuidado que a primera vista puede parecer que la funcion necesaria es una transformacion `map()`, pero si piensas un poco mas sobre el resultado de la funcion `split()` te daras cuenta que esta no es la opcion correcta.\n\n> Nota:\n> * No uses la implementacion estandar del `split()`, pasale un valor de separacion. Por ejemplo, para dividir `line` por comas, usa `line.split(',')`."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nshakespeareWordsRDD = shakespeareRDD.flatMap(lambda line: line.split(' '))\nshakespeareWordCount = shakespeareWordsRDD.count()\nprint shakespeareWordsRDD.top(5)\nprint shakespeareWordCount"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# TEST Words from lines (4d)\n# This test allows for leading spaces to be removed either before or after\n# punctuation is removed.\nTest.assertTrue(shakespeareWordCount == 927631 or shakespeareWordCount == 928908,\n                'incorrect value for shakespeareWordCount')\nTest.assertEquals(shakespeareWordsRDD.top(5),\n                  [u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds'],\n                  'incorrect value for shakespeareWordsRDD')"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["### (4e) Elimina los elementos vacios\n\nEl siguiente paso es eliminar los espacios vacios. Elimina todas las entradas donde la palabra sea `''`."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nshakeWordsRDD = shakespeareWordsRDD.filter(lambda w: w.strip()!='')\nshakeWordCount = shakeWordsRDD.count()\nprint shakeWordCount"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# TEST Remove empty elements (4e)\nTest.assertEquals(shakeWordCount, 882996, 'incorrect value for shakeWordCount')"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### (4f) Cuenta las palabras\n\nAhora que tenemos un RDD que contiene solo palabras. El siguiente paso es aplicar la funcion `wordCount()` para producir una lista con los conteos de palabras. Podemos ver las 15 mas comunes usando la accion `takeOrdered()`; sin embargo, como los elementos del RRD son pares, necesitamos una funcion especial que ordene los pares de la forma correcta.\n\nUsa las funciones  `wordCount()` y `takeOrdered()` para obtener las 15 palabras mas comunes junto con sus conteos."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ntop15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key = lambda x: -x[1])\nprint '\\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# TEST Count the words (4f)\nTest.assertEquals(top15WordsAndCounts,\n                  [(u'the', 27361), (u'and', 26028), (u'i', 20681), (u'to', 19150), (u'of', 17463),\n                   (u'a', 14593), (u'you', 13615), (u'my', 12481), (u'in', 10956), (u'that', 10890),\n                   (u'is', 9134), (u'not', 8497), (u'with', 7771), (u'me', 7769), (u'it', 7678)],\n                  'incorrect value for top15WordsAndCounts')"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["## Parte 5: Calcular algunos estadisticos\n\nAhora que en shakeWordsRDD tenemos un RDD que contiene solo palabras, vamos a intentar estudiar algunas caracteristicas de la lengua inglesa. Usando las mismas tecnicas que has aplicado en los ejercicios anteriores responde a las siguientes preguntas:\n\n- Cual es la letra del alfabeto por la que empiezan mas palabras diferentes? Nota: Puedes usar un pair RDD o el metodo _distinct_ para eliminar los duplicados\n- Cuantas palabras contienen la letra 'a'?\n- Cual es la longitud media de las palabras de las obras de W. Shakespeare?"],"metadata":{}},{"cell_type":"code","source":["#TODO: write all the required code to answer the aforementioned questions\n\n#Cual es la letra del alfabeto por la que empiezan mas palabras diferentes?\n#step 5.a -> aplicar la función wordCount para agrupar las palabras repetidas, en este paso podemos extraer tambien la primera letra\nintialLetters = wordCount(shakeWordsRDD).map(lambda w: w[0][0])\n#step 5.b -> volvemos a aplicar la funcion wordCount para contar las letras y devolvemos la letra con el contador más alto\nprint \"la letra del alfabeto por la que empiezan mas palabras diferentes es \"\nprint wordCount(intialLetters).takeOrdered(1, key = lambda w: -w[1])"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["#Cuantas palabras contienen la letra 'a'?\n#step 5.b -> Si queremos contar las palabras incluyendo las repetidas seria\nprint shakeWordsRDD.filter(lambda w: 'a' in w).count()\n#step 5.b -> Si queremos contar las palabras sin repetidas seria\nprint wordCount(shakeWordsRDD).filter(lambda w: 'a' in w[0]).count()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["#Cual es la longitud media de las palabras de las obras de W. Shakespeare?\nshakeWordsCount = wordCount(shakeWordsRDD)\nprint shakeWordsCount.map(lambda w : len(w[0])).reduce(lambda a,b : a+b) / shakeWordsCount.count()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":59}],"metadata":{"name":"Lab_1_WordCount","notebookId":1834615363821281},"nbformat":4,"nbformat_minor":0}
