{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data II (part 1)\n",
    "\n",
    "In \"Big Data I\" lesson we've seen multicore programming. That is, taking advantage of multiple cores in a computer to speed up our code.  \n",
    "In \"Big Data II\" we want to focus on what can we do to go further. How to increase the amount of data we can manage by unit of time.  \n",
    "\n",
    "We are going to introduce multiple key concepts: **scale-up (supercomputers)** and **scale-out (cluster/grid computing)** and, finally, we will see that not every solution is parallelizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.- Parallelization\n",
    "\n",
    "When working with big data is important to pay attention to throughput; that is reducing computational time at maximum for each operation on data. When working with millions of registers, improving 1µs in computation time for each register is significant.  \n",
    "\n",
    "What can we do to improve our code execution? \n",
    "\n",
    "+ Buy new/more hardware. \n",
    "+ Program using all hardware resources. \n",
    "       \n",
    "As we have seen in \"Big Data I\" lesson, multicore programming permits us to take advantage of all resources.\n",
    "That is **\"Divide and conquer\"**: divide a problem in smaller parts and deal with each part individually.\n",
    "This idea can be used in computing to speed up some kind of code (parallelizable code)\n",
    "\n",
    "### Multi-core parallelization\n",
    "In **\"Big Data I\"** lesson we have seen how to use **ipcluster** command to take advantage of all the cores in a computer. Let's see a short review.\n",
    "\n",
    "#### Start the cluster\n",
    "Using the \"Clusters\" Tab, we can create and start a cluster of engines in your computer. We can see the processes related to ipyparallel in our computer with a command like:  \n",
    ">*top -p \\`pgrep -f ipyparallel -d \",\"\\` -c*\n",
    "\n",
    "#### Connect to the cluster\n",
    "Using 'parallel' package, you can connect to the cluster\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ipyparallel as ipp\n",
    "\n",
    "clients = ipp.Client()\n",
    "clients.block = True\n",
    "clients.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the engines\n",
    "With above commands, we have started N engines that can be used in parallel. That is: we have N python interpreters that can be used in parallel.\n",
    "\n",
    "We have two primary models for interacting with engines:  \n",
    "\n",
    "**Direct interface**: Send instructions to engines explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples taken form \"Big Data I\" lesson\n",
    "#Send direct commands to multiple engines:\n",
    "clients[0].execute('a = 2')\n",
    "clients[0].execute('b = 10')\n",
    "clients[1].execute('a = 9')\n",
    "clients[1].execute('b = 7')\n",
    "clients[0:2].execute('c = a + b')\n",
    "\n",
    "#Get the result from the engines\n",
    "print(clients[0:2].pull('c'))\n",
    "\n",
    "#We can acces multiple engines at a time using a variable:\n",
    "dview2 = clients[0:2]\n",
    "noResultExpected = dview2.execute('reset -f') #This instruction cleans the interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoadBalanced View**: The cluster decides which engines must be used in a balanced strategy (SGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Our cluster has {0} engines/nodes. Their ids are: {1}\".format(len(clients.ids), clients.ids))\n",
    "\n",
    "#With blocking option, our interpreter will be blocked until all engines have received all values.\n",
    "# Avoids that engine N recives its ID after receiving its task. More info on this in \"Big Data I\" notebook\n",
    "clients.block = True\n",
    "\n",
    "#Create a loadBalanced view\n",
    "lview = clients.load_balanced_view()\n",
    "\n",
    "#With execute, we send a single instruction to each engine.\n",
    "# Sets a diferent var \"my_id\" in each engine\n",
    "for i in clients.ids:     #i=0,1,2,3 in a 4 nodes cluster.\n",
    "    clients[i].execute('my_id = ' + str(i), block=False)\n",
    "    \n",
    "#Define a function that recives a parameter, does \"some work\" with it,\n",
    "#  and returns the value of \"my_id\" var:\n",
    "def sleep_and_return_id(sec):\n",
    "    import time\n",
    "    #Do something very interesting here, or just sleep\n",
    "    time.sleep(sec)\n",
    "    return \"engine id:\" + str(my_id) + \" waited for \" + str(sec) + \" seconds\"\n",
    "\n",
    "#With the map method, we send the function and data to be managed by the LoadBalanced View\n",
    "lview.map(sleep_and_return_id, [1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Scale-up and scale-out\n",
    "With with this, we have reviewed how to use each of our N cores to speedup the code execution.\n",
    "We have reduced the compute time from *t* to something near *t*/N.\n",
    "\n",
    "What can we do if we want to reduce this time even more? How can we scale our computational system?\n",
    "\n",
    "+ One solution might be what is called **scale-up**: buy a new computer or a new processor with more cores, add more RAM, buy new storage, and so on.\n",
    "+ Another solution is called **scale-out**: buy commodity computers, interconnect them and make them to operate together to solve your problem. That is: create a **Cluster** (hundreds of nodes in an enterprise) or a **GRID** (user computers interconected)\n",
    "\n",
    "The main idea is:\n",
    "+ **Scale-up** gives you the best performance values, but have limitations on scalability.\n",
    "+ **Scale-out** gives you a great scalability, but performance is worst.\n",
    "\n",
    "In http://www.top500.org/ you can see top 500 supercomputers/HPC (High Performance Computing) worldwide. Since  2016/6, the top HPC is **Sunway TaihuLight** (https://www.top500.org/site/50623) a supercomputer with **10,649,600 cores**, 93PFlops that needs **15,371kW** to work\n",
    "\n",
    "Sunway TaihuLihgt has, on april 2016, the highest performance registered in a single system; besides the price, this kind of systems have other limitations:\n",
    "\n",
    "+ If possible, it is very expensive to scale the system.\n",
    "+ It wastes a lot of power, even if you want to execute a simple instruction.\n",
    "\n",
    "With cluster/grid computing you can scale your system to meet your needs: add as many nodes as you need, use all of them or only a part of them.  \n",
    "One of most prominent examples of grid computing is SETI@home: a project for searching extraterrestial life using user's computers; the GRID computes ~22PFlops (~855,000 computers)\n",
    "\n",
    "When working with grid computing, we must keep in mind its important limitations:\n",
    "\n",
    "+ Computers are interconnected using internet: it is slow and subject to failures.\n",
    "+ Computers are distant. It's impossible to share memory and it's difficult to share a FileSystem. Data movement slowdown must be taken into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- How to get a Cluster? In the cloud, where else?\n",
    "\n",
    "Three major providers\n",
    "Amazon EC2 - https://aws.amazon.com/ec2/\n",
    "Microsoft Azure - https://azure.microsoft.com/es-es/\n",
    "Google Cloud Platform - https://cloud.google.com/\n",
    "\n",
    "We are going to see an example on Amazon EC2 because, on 2017, it has a 40% share market.\n",
    "\n",
    "### Amazon EC2 - Amazon Elastic Compute Cloud ###\n",
    "\n",
    "\"Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.\"\n",
    "\n",
    "In few words: **Amazon EC2 gives us the possibility to have a grid in the cloud.**  \n",
    "Amazoon EC2 has a cost by hour, according to the OS, the CPU, memory, storage and so on that we contract. \n",
    "There is a calculator in http://calculator.s3.amazonaws.com/index.html to get the cost estimation of our infraestructure.\n",
    "\n",
    "When signin in, the service gives you a **Free Tier**, with develop purposes. This gives us, with restrictions,  **750h/month** of computing for free during one year. That would be enough for develop purposes.\n",
    "\n",
    "Keep in mind some restrictions of the Free Tier:\n",
    "\n",
    "+ 750h/month during one year.\n",
    "+ Only includes t2.micro instances\n",
    "+ Only includes some OS\n",
    "+ There is a limit in amonut of storage\n",
    "+ There is a limit on the traffic input/output\n",
    "\n",
    "For more info: http://aws.amazon.com/free/\n",
    "\n",
    "At last, keep in mind that sign in implies that Amazon have your credit card number. If you commit an error, you'll have charges in you credit card. Most common errors are:\n",
    "\n",
    "+ Using instances or OS that are not elegible by the Free Tier.\n",
    "+ Not terminating instances: 750h with 100 nodes gives you \"only\" 7.5h for free.\n",
    "+ Reserving a public IP and not ocupying it.\n",
    "\n",
    "Last, but not least important: be carefull with your credentials. With your EC2 credentials, someone could start nodes that will be charged in your credit card. Be specially carefull introducing your credentials into world readable sites (GitHub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the grid\n",
    "\n",
    "### The hard way\n",
    "\n",
    "Ipcluster command can be used to create a **grid** of computers in Amazon EC2.\n",
    "\n",
    "The problem with this method is that there is a lot of hard work to do to make it operational:\n",
    "\n",
    "We may create instances in Amazon EC2 and configure them:\n",
    "\n",
    "+ Install ipython\n",
    "+ Defining a security police\n",
    "+ Interconnect them\n",
    "+ We may want to create a shared filesystem\n",
    "\n",
    "This is a hard way. If you have your own grid and you are interested on using ipcluster, you must start reading **Starting the IPython controller and engines**[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The engineer way: StarCluster[2]\n",
    "\n",
    "http://star.mit.edu/cluster/  \n",
    "\"StarCluster is an open source cluster-computing toolkit for Amazon’s Elastic Compute Cloud (EC2) released under the LGPL license.   \n",
    "StarCluster has been designed to automate and simplify the process of building, configuring, and managing clusters of virtual machines on Amazon’s EC2 cloud. StarCluster allows anyone to easily create a cluster computing environment in the cloud suited for distributed and parallel computing applications and systems.\"\n",
    "\n",
    "http://star.mit.edu/cluster/docs/latest/overview.html  \n",
    "Out-of-the-box, StarCluster automates:  \n",
    "\n",
    "+ Create a security police for the cluster (Firewall)\n",
    "+ User-friendly Hostnames: master, node001, node002, etc\n",
    "+ Comunication bw nodes: SSH + NFS share on /home\n",
    "+ Creates a Queuing system (Oracle Grid Engine)\n",
    "\n",
    "#### First of all: non-human interaction with Amazon EC2 API\n",
    "\n",
    "There are two main ways to operate with AWS: using the **web portal** or in a non-human interaction using the **Amazon API**.  \n",
    "As we want Starcluster to do the hard work automatically, we must use Amazon API.\n",
    "\n",
    "Using Amazon API requires an User with an Access Key ID and a Secret Access Key. To create them:  \n",
    "\n",
    "+ In AWS, go to IAM (Identify & Access Management) -> In Users section, choose the user or create a new one. The user needs \"Programatic Access\". Download the Access Key ID and the Secret Access Key and store them.\n",
    "\n",
    "For this class, in the Policies section, we are going to authorize our user to have AdministratorAccess.\n",
    "\n",
    "#### Installing Starcluster\n",
    "\n",
    "http://star.mit.edu/cluster/docs/latest/installation.html  \n",
    "On Linux/Mac/Windows: 'pip install StarCluster'\n",
    "\n",
    "#### Create a small cluster with StarCluster\n",
    "\n",
    "To create a new **configuration file**, in a command line, execute:  \n",
    "    \n",
    "     $> starcluster help #Choose option 2\n",
    "\n",
    "Edit the configuration file **(~/.starcluster/config)** and set your **AWS authentication settings**:  \n",
    "\n",
    "      [aws info]\n",
    "      AWS_ACCESS_KEY_ID = your-acces-key-id\n",
    "      AWS_SECRET_ACCESS_KEY = your-secret_access_key\n",
    "      AWS_USER_ID = your-user_id (User name!)\n",
    "      \n",
    "To create the cluster, StarCluster needs to log in into the diferent nodes in EC2 without providing a password.  \n",
    "With this purpose, Starcluster uses an RSA **keipair keys**. We can create and activate a keypair with the command:\n",
    "\n",
    "    $> starcluster createkey -o bigDataKeyPair.rsa bigDataKeyPair\n",
    "    \n",
    "Be sure to have 600 permissions on the keypair file!  \n",
    "\n",
    "    $> chmod 600 bigDataKeyPair.rsa\n",
    "\n",
    "After this, we can set the keypair into Starcluster configuration file **(~/.starcluster/config)**:\n",
    "\n",
    "      #Set your keypairs, used to connect to your instances. We are going to create them in a moment\n",
    "      [key bigDataKeyPair] #Must be the same name you created!\n",
    "      KEY_LOCATION=/path/to/your/bigDataKeyPair.rsa\n",
    "      \n",
    "And finally, setup our cluster:  \n",
    "\n",
    "      #Cluster settings\n",
    "      [cluster smallcluster]\n",
    "      KEYNAME = bigDataKeyPair #This is the same specified in [keipair] section\n",
    "      CLUSTER_SIZE = 5 #Number of nodes to be started\n",
    "      PLUGINS = ipcluster #To enable ipython\n",
    "      #\n",
    "      #Ipcluster plugin settings\n",
    "      [plugin ipcluster]\n",
    "      #Enable next lines if you want the cluster to start a new ipython interpreter\n",
    "      # Be sure to modify permission to grant https acces to your cluster\n",
    "      #enable_notebook = True\n",
    "      #notebook_passwd = SuperSecretPassword    \n",
    "         \n",
    "#### Start the cluster:\n",
    "\n",
    "To start the cluster, from command line, execute the command:\n",
    "\n",
    "     $> starcluster start smallcluster\n",
    "     \n",
    "More info on **[3] - IPython Cluster Plugin**\n",
    "\n",
    "#### Using the cluster\n",
    "The following command creates and connects to a python interpreter in the master node using SSH:\n",
    "\n",
    "    $>starcluster sshmaster smallcluster -u sgeadmin\n",
    "    \n",
    "When inside the master node, we can start an ipython interpreter and start working with the cluster.\n",
    "\n",
    "    In [1]: from IPython.parallel import Client\n",
    "    In [2]: rc = Client()\n",
    "    In [3]: rc.ids\n",
    "    Out[4]: [0, 1, 2, 3, 4]\n",
    "    In [4]: %px !ifconfig eth0 | grep 'inet addr'\n",
    "    [stdout:0]           inet addr:10.232.96.237  Bcast:10.232.96.255  Mask:255.255.255.192\n",
    "    [stdout:1]           inet addr:10.170.57.47  Bcast:10.170.57.63  Mask:255.255.255.192\n",
    "    [stdout:2]           inet addr:10.170.90.68  Bcast:10.170.90.127  Mask:255.255.255.192\n",
    "    [stdout:3]           inet addr:10.171.4.251  Bcast:10.171.4.255  Mask:255.255.255.192\n",
    "    [stdout:4]           inet addr:10.170.58.209  Bcast:10.170.58.255  Mask:255.255.255.192\n",
    "    \n",
    "You can copy files between your local computer and the cluster, using put and get commands: \n",
    "\n",
    "    starcluster put mycluster /local/path/file /remote/path/  \n",
    "    starcluster get smallcluster /remote/path/results /local/path\n",
    "    \n",
    "#### Stop and Terminate cluster\n",
    "\n",
    "When terminating an instance, Amazon stops the instance and then deletes all data stored in it.\n",
    "To stop the cluster:\n",
    "\n",
    "    $> starcluster stop smallcluster\n",
    "    \n",
    "To terminate the cluster (stop and deletes all data) you can use:\n",
    "\n",
    "    $> starcluster terminate smallcluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The way: SaaS (Software as a Service) Amazon EMR\n",
    "\n",
    "Cloud providers launched services like:\n",
    "+ **Elastic Cache**. Amazon's service to create clusters.\n",
    "+ **EMR**. Amazon's service to create MapReduce clusters.\n",
    "+ **HDInsights**. Microsoft's service to create linux based clusters in Azure (Hadoop and so on)\n",
    "+ **Google Cloud Dataproc**. Google's service to create hadoop clusters to operate with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final notes on data movement and MapReduce    \n",
    "\n",
    "Something important to take into account when working with distributed computing is **data movement**. Where do you have your data? How can your nodes access data?  \n",
    "\n",
    "One choice is having a shared filesystem, like we have in Amazon or other cloud providers. There are some problems with shared filesystem are:\n",
    "\n",
    "+ Dificult to implement with computers in diferent networks (providers).\n",
    "+ Data is **stored in a server**. Each node has to connect with the server and retrieve data from the same server. It can be a bottleneck when working with **big data**\n",
    "\n",
    "Here, we want to introduce the concept of **distributed filesystems**[4]. The idea behind them is: divide your filesystem in chunks and replicate each chunk in multiple nodes. With this solution you get:\n",
    "\n",
    "+ Your data is distributed: the same node that stores the chunk can work with it.\n",
    "+ Your data is replicated, so there is no single point of failure.\n",
    "\n",
    "This is the idea behind MapReduce:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce?\n",
    "\n",
    "At this point, we want to mention one word that you might have listened once introduced into Big Data world: **MapReduce**.\n",
    "\n",
    "From Wikipedia [6]:  \n",
    "*\"MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster.\"*   \n",
    "  \n",
    "In this lesson we are not going in depth on MapReduce, but we want you to keep one key concept: **\"MapReduce is not only Map and Reduce\"**.\n",
    "\n",
    "Python, as many other languages, has map and reduce functions:  \n",
    "\n",
    "+ map(function, iterable)  - Applies a 'function' to each item of 'iterable' and returns a new iterable with the results.\n",
    "+ reduce(function, iterable [,initializer]) - Applies a cumulative 'function' to the items of iterable, from left to right, so as to reduce the iterable to a single value.\n",
    "\n",
    "Let's see a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "#Maps the square function to each element of a list\n",
    "mapResults = map(lambda x: x**2, range(1,11))\n",
    "\n",
    "#Reduce mapResults to get the total sum\n",
    "reduceResults = functools.reduce(lambda x, y: x+y, mapResults)\n",
    "\n",
    "#Show the results\n",
    "print(reduceResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MapReduce** is an entire framework that simplifies our lifes in the way that programmers only have to create Map and Reduce functions. MapReduce does the rest of the work:\n",
    "\n",
    "+ Uses a distributed filesystem, shares data between the nodes (Data Movement)\n",
    "+ Applyes Map and Reduce functions to the data\n",
    "+ Returns reduce results to the users\n",
    "+ Manage nodes failures\n",
    "+ ...\n",
    "\n",
    "Some well known implementations of MapReduce are:\n",
    "\n",
    "+ Google MapReduce cluster [7]\n",
    "+ Hadoop [8]\n",
    "+ MongoDB [9]\n",
    "\n",
    "Is interesting to read the Google papers, publicated on 2003 and 2004, that revolutionazed distributed computing. Both are public:\n",
    "\n",
    "+ Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung from Google. oct 2003. **The Google File System** <http://research.google.com/archive/gfs.html>  \n",
    "+ Jeffrey Dean and Sanjay Ghemawat from Google. dec 2004. **MapReduce: Simplified Data Processing on Large Clusters** <http://research.google.com/archive/mapreduce.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources\n",
    "\n",
    "[1] Starting the IPython controller and engines: http://ipython.org/ipython-doc/2/parallel/parallel_process.html#using-ipcluster  \n",
    "[2] StarCluster - http://star.mit.edu/cluster/index.html  \n",
    "[3] IPython Cluster Plugin - http://star.mit.edu/cluster/docs/latest/plugins/ipython.html  \n",
    "[4] Wikipedia Distributed Fylesystems - http://en.wikipedia.org/wiki/Clustered_file_system#Distributed_file_systems  \n",
    "[5] Wikipedia Apache Hadoop - http://en.wikipedia.org/wiki/Apache_Hadoop  \n",
    "[6] Wikipedia MapReduce - http://en.wikipedia.org/wiki/MapReduce  \n",
    "[7] Google MapReduce cluster - http://static.googleusercontent.com/media/research.google.com/ca//archive/mapreduce-osdi04.pdf  \n",
    "[8] Hadoop - http://hadoop.apache.org/  \n",
    "[9] MongoDB - http://www.mongodb.org/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
