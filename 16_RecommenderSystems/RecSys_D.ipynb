{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<small><i>Updated January 2020 - This notebook was created by [Santi Seguí](https://ssegui.github.io/). </i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;\">\n",
    "<h1>Recommenders Systems</h1>\n",
    "<h3>Item-Based Recommender System</h3>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Item-Based Recommmender Systems is a special type of CF-Recommender Systems.\n",
    "Instead on relying on the user similarity, prediction can rely on item similarities.\n",
    "\n",
    "<img src=\"images/neighbourhood.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The similarity between two users is measured as a tendecy of measuring items simiarly. Pearson or Cosine distance are usually used.\n",
    "### Similarity Functions\n",
    "\n",
    "The computation of the similarity between the items is critical. Similar to the procedure done when comparing users. The similarity computation between two items <i>p</i> and <i>q</i> can be obtained by first isolating the users that have both rated the items (set <i>U</i>), and then then apply a distance or correlation function.\n",
    "\n",
    "<ul>\n",
    "    <li>Euclidean distance</li>\n",
    "    $$sim(p,q) = \\sqrt{\\sum_{u \\in P}{(r_{u,p} - r_{u,q})^2}}$$\n",
    "    <br>\n",
    "    <li>Pearson Correlation</li>\n",
    "    $$sim(p,q) = \\frac{\\sum_{u\\in U} (r_{u,p}-\\bar{r_p})(r_{u,q}-\\bar{r_q})}{\\sqrt{\\sum_{u \\in U}(r_{u,p}-\\bar{r_p})^2}\\sqrt{\\sum_{u\\in U}(r_{u,q}-\\bar{r_q})^2}}$$\n",
    "    <br>\n",
    "    <li>Cosine distance</li>\n",
    "    $$ sim(p,q) = \\frac{\\vec{p}· \\vec{q}}{|\\vec{p}| * |\\vec{q}|}$$\n",
    "    <br>\n",
    "    </ul>\n",
    "<br>\n",
    "Where: \n",
    "\n",
    "* $sim(p,q)$ is the similarity between items \"p\" and user \"q\"\n",
    "* $U$ is the set of users that have both rated \"p\" and \"q\"\n",
    "* $r_{u,p}$ is the rating of movie \"p\" by user \"u\"\n",
    "* $\\bar{r_p}$ is the mean rating of product \"p\"\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WARNING : \n",
    "Computing similarity using basic cosine measure in item-based case has one important drawback: The differences in rating scale between different users are not taken into account.\n",
    "The Adjusted Cosine Similarity offsets this drawback by subtracting the corresponding user average from each co-rated pair:\n",
    "$$ sim(p,q) = \\frac{\\sum_{u \\in U} (r_{u,p} - \\bar{r}_u) (r_{u,j} -\\bar{r}_u)}{\\sqrt{ \\sum_{u \\in U}  (r_{u,p} - \\bar{r}_u)^2}\\sqrt{ \\sum_{u \\in U}  (r_{u,q} - \\bar{r}_u)^2} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction Function\n",
    "\n",
    "$$pred(u,p) =  \\frac{\\sum_{q \\in M}{sim(p,q)*(r_{u,q})}}{\\sum_{q \\in M}{sim(p,q)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(150000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 150 seconds\n",
      "La BD has 100000 ratings\n",
      "La BD has  943  users\n",
      "La BD has  1682  movies\n",
      "(14384, 7)\n",
      "# Users: 805\n",
      "# Movies: 99\n"
     ]
    }
   ],
   "source": [
    "#NETFLIX REAL 50.000.000 usuaris and 100.000 items\n",
    "%autosave 150\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Load Data set\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('ml-100k/u.user', sep='|', names=u_cols)\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=r_cols)\n",
    "\n",
    "# the movies file contains columns indicating the movie's genres\n",
    "# let's only load the first three columns of the file with usecols\n",
    "m_cols = ['movie_id', 'title', 'release_date']\n",
    "movies = pd.read_csv('ml-100k/u.item', sep='|', names=m_cols, usecols=range(3), encoding='latin-1')\n",
    "\n",
    "# Construcció del DataFrame\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "data = data[['user_id','title', 'movie_id','rating','release_date','sex','age']]\n",
    "\n",
    "\n",
    "print(\"La BD has \"+ str(data.shape[0]) +\" ratings\")\n",
    "print(\"La BD has \", data.user_id.nunique(),\" users\")\n",
    "print(\"La BD has \", data.movie_id.nunique(), \" movies\")\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# let's define some functions\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "def assign_to_set(df):\n",
    "    sampled_ids = np.random.choice(df.index,\n",
    "                                   size=np.int64(np.ceil(df.index.size * 0.2)),\n",
    "                                   replace=False)\n",
    "    df.loc[sampled_ids, 'for_testing'] = True\n",
    "    return df\n",
    "\n",
    "def create_train_test(data,key = 'user_id'):\n",
    "    data['for_testing'] = False\n",
    "    grouped = data.groupby(key, group_keys=False).apply(assign_to_set)\n",
    "    # dataframe used to train our model\n",
    "    data_train = data[grouped.for_testing == False]\n",
    "    # dataframe used to evaluate our model\n",
    "    data_test = data[grouped.for_testing == True]\n",
    "    return data_train, data_test\n",
    "\n",
    "def evaluate(estimate_f,data_train,data_test):\n",
    "    \"\"\" RMSE-based predictive performance evaluation with pandas. \"\"\"\n",
    "    ids_to_estimate = zip(data_test.user_id, data_test.movie_id)\n",
    "    estimated = np.array([estimate_f(u,i) if u in data_train.user_id else 3 for (u,i) in ids_to_estimate ])\n",
    "    real = data_test.rating.values\n",
    "    return compute_rmse(estimated, real)\n",
    "\n",
    "\n",
    "# In order to speed up the test let's create a tiny dataset\n",
    "dataSmall = data[data['movie_id']<100]\n",
    "print(dataSmall.shape)\n",
    "\n",
    "dataSmall_train, dataSmall_test =  create_train_test(data)\n",
    "\n",
    "print('# Users:', dataSmall.user_id.nunique())\n",
    "print('# Movies:',dataSmall.movie_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**EXERCISE: Create an item-based recommender system. **\n",
    "<br>Instead of computing similiarity between users we need to compute similarities between items. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Returns a pearsonCorrealation-based similarity score for person1 and person2\n",
    "def SimPearson(DataFrame,User1,User2,min_common_items=1):\n",
    "    # GET MOVIES OF USER1\n",
    "    movies_user1=DataFrame[DataFrame['user_id'] ==User1 ]\n",
    "    # GET MOVIES OF USER2\n",
    "    movies_user2=DataFrame[DataFrame['user_id'] ==User2 ]\n",
    "    \n",
    "    # FIND SHARED FILMS\n",
    "    rep=pd.merge(movies_user1 ,movies_user2,on='movie_id',)\n",
    "    if len(rep)==0:\n",
    "        return 0    \n",
    "    if(len(rep)<min_common_items):\n",
    "        return 0    \n",
    "    res=pearsonr(rep['rating_x'],rep['rating_y'])[0]\n",
    "    if(isnan(res)):\n",
    "        return 0\n",
    "    return res\n",
    "\n",
    "# Returns a pearsonCorrealation-based similarity score for movie1 and movie2\n",
    "def SimPearsonItem(DataFrame,movie1,movie2,min_common_items=1):\n",
    "    # GET USER THAT HAVE SEEN MOVIE1\n",
    "\n",
    "    # GET USER THAT HAVE SEEN MOVIE2\n",
    "    \n",
    "    # FIND USERS THAT HAVE SEEN BOTH OF THEM\n",
    "    \n",
    "    # COMPUTE AND RETURN ITEM SIMILARITY\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm as tq # conda install -y tqdm\n",
    "\n",
    "class CollaborativeFilteringItemBased:\n",
    "    \"\"\" Item-Based Collaborative filtering using a custom sim(p,p'). \"\"\"\n",
    "    \n",
    "    def __init__(self,DataFrame, similarity=SimPearsonItem, min_common_items=10, max_sim_movies=10):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.sim_method=similarity# Gets recommendations for a person by using a weighted average\n",
    "        self.df=DataFrame\n",
    "        self.sim = pd.DataFrame(np.sum([0]), columns = DataFrame.movie_id.unique(), index = DataFrame.movie_id.unique())\n",
    "        self.min_common_items=min_common_items\n",
    "        self.max_sim_movies=max_sim_movies\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\" Prepare data structures for estimation. Similarity matrix for users \"\"\"\n",
    "        allMovies=set(self.df['movie_id'])\n",
    "        self.sim = {}\n",
    "        pbar = tq.tqdm(total=len(allMovies))\n",
    "        for movie1 in allMovies:\n",
    "            self.sim.setdefault(movie1, {})\n",
    "            a=self.df[self.df['movie_id']==movie1][['user_id']]\n",
    "            data_reduced=pd.merge(self.df,a,on='user_id')\n",
    "            for movie2 in allMovies:\n",
    "                # no es comparem am nosalres mateixos\n",
    "                if movie1==movie2: continue\n",
    "                self.sim.setdefault(movie2, {})\n",
    "                if(movie1 in self.sim[movie2]):continue # since is a simetric matrix\n",
    "                sim=self.sim_method(data_reduced,movie1,movie2,self.min_common_items)\n",
    "                #print movie1,movie2,sim\n",
    "                if(sim<0):\n",
    "                    self.sim[movie1][movie2]=0\n",
    "                    self.sim[movie2][movie1]=0\n",
    "                else:\n",
    "                    self.sim[movie1][movie2]=sim\n",
    "                    self.sim[movie2][movie1]=sim  \n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "                \n",
    "    def estimate(self, user_id, movie_id):\n",
    "        if movie_id not in self.df.movie_id.unique(): # If the movie was not in the training set\n",
    "            return 3.5\n",
    "        totals={}\n",
    "        movie_users=self.df[self.df['user_id'] ==user_id]\n",
    "        rating_num=0.0\n",
    "        rating_den=0.0\n",
    "        allMovies=set(movie_users['movie_id'])\n",
    "        \n",
    "        listOrdered=sorted([(self.sim[movie_id][other],other) for other in allMovies if movie_id!=other],reverse=True)\n",
    "        \n",
    "        for item in range(min(len(listOrdered),self.max_sim_movies)):\n",
    "            other=listOrdered[item][1]\n",
    "            rating_num += self.sim[movie_id][other] * (float(movie_users[movie_users['movie_id']==other]['rating']))\n",
    "            rating_den += self.sim[movie_id][other]\n",
    "        if rating_den==0: \n",
    "            if self.df.rating[self.df['movie_id']==movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.df.rating[self.df['movie_id']==movie_id].mean()\n",
    "            else:\n",
    "                # else return mean user rating \n",
    "                return self.df.rating[self.df['user_id']==user_id].mean()\n",
    "        return rating_num/rating_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [00:12<00:00, 127.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.745614035087719"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reco = CollaborativeFilteringItemBased(dataSmall_train,similarity=SimPearsonItem,min_common_items=1,max_sim_movies=10)\n",
    "reco.learn()\n",
    "reco.estimate(user_id=2,movie_id=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Collaborative Recomender: 1.0794258283104319\n"
     ]
    }
   ],
   "source": [
    "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.estimate,dataSmall_train,dataSmall_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advantatges\n",
    "\n",
    "+ Item similarity used to be <b>more stable</b> than user-similarity, \n",
    "+ More scalable. The update frequency of the items similarity is not as critical than user-similarity since it is more stable\n",
    "+ It uses to perform better in RMSE terms, while being also more scalable. <br>\n",
    "\n",
    "Both advantages are related with the fact that there is usually less users than item.<br><br>\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "* It tends to recommend obvoius items. It is very difficult for the item-based method to discover highly different items to recommend. \n",
    "+ Why: Similarities are computed with more data. The evidence of non-popular items its really complex since there is no evidence on the similarity.\n",
    " + May be frustating for browsing/enterteiment \n",
    " + Can be good for shopping, consumption taks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;\">\n",
    "<h3>Latent Factor Models.</h3>\n",
    "</div>\n",
    "\n",
    "Singular Value Decomposition (SVD) is another well known method for recommender systems.\n",
    "The key idea fo the SVD model is to factorize the user-item matrix rating matrix into two lower rank matrices, one containing the user factors, while the other containing the  item factors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality Reduction and Neighbourhood Methods\n",
    "\n",
    "Dimensionality reduction can improve neighborhood methods in terms of accuracy and also in terms of efficiency.\n",
    "\n",
    "\n",
    "* A reduced representation of the data can be created in terms of either row-wise latent factors or column-wise latent factors. In other, words, the reduced representation will compress items or users into latent factors. \n",
    "* Can alleviate the sparsity problems for neigbborhood-based models.\n",
    "* Depending on which represetations is compressed, it can be used as user-based or item-based neigborhood model.\n",
    "* The latent representation can be also computed in <b>both</b> dimensions simultaneously. The full $m \\times n$ matrix is obtained in one shot without the use of neigborhood-based methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Latent Factor Models**:\n",
    "![alt svd](images/svd.png)\n",
    "\n",
    "$$ R = UV^T$$\n",
    "$$ \\hat{R} \\approx  UV^T$$\n",
    "$$ \\hat{r}_{i,j} \\approx  \\hat{u}_i\\cdot\\hat{v}_j$$\n",
    "$$\\sum_{s=1}^{k}\\text{(Affinity of user i to concept S)}\\cdot\\text{(Affinity of item j to concept S)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, the idea is to found topics as follows:\n",
    "![alt svd](images/svd2.png)\n",
    "This representation can be computed using standard methods like <b>PCA</b> or <b>SVD</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the purpose of the recommendation systems, we can also be interested in the matrix factorization that keeps the same dimensionality. It can be understood as a model-based recommender system.\n",
    "\n",
    "The matrix factorization is done on the user-item ratings matrix $R$. From a high level, matrix factorization can be thought of as finding 2 matrices whose product is the original matrix:\n",
    "$$ \\hat{R} = Q^TP$$\n",
    "where:\n",
    "$$ r_{u,i} = q^T_ip_u$$\n",
    "\n",
    "\n",
    "The goal of the method consist of optimizing the matrices $Q$ and  $P$. It can be done as follows:\n",
    "$$ minimize_{Q,P} \\sum_{u,i \\in K} (r_{u,i} - q^t_ip_u )^2$$\n",
    "\n",
    "To generalize well and not over-fit the training set, a penalty term is usually introduced into the minimization equation. This is represented by a regularization factor $\\lambda$ multiplied by the square sum of the magnitudes of user and item vectors.\n",
    "\n",
    "$$ minimize_{Q,P} \\sum_{u,i \\in K} (r_{u,i} - q^t_ip_u )^2 + \\lambda(|| q_i||^2 + || p_u||^2)$$\n",
    "\n",
    "\n",
    "\n",
    "Ok, but... what should we do with the huge amount of unknow values? \n",
    "* Traditionally, the average rating of the movies was used for all unkown items.\n",
    "\n",
    "And, what happens if the number of users and item is large? \n",
    "* The factorization of the matrix becomes harder and the solution is not ensured to be the optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVD\n",
    "![alt svd](images/svd4.png)\n",
    "* Columns of U and V are constrained to be mutually orthogonal. \n",
    "* Mutual orthogonality has the advantage that the concepts can be completely independent of one another. Can be interpreted in scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Studies showed that if the evaluation is done with <b>Top-N recommendations</b> metric unkown values can be set to 0. In this way we obtain a sparse user-item matrix. This is an important point since the comutation of the SVD is easier and faster on sparse matrices.\n",
    "\n",
    "While similarities are hard to be computed in huge dimensional sparse rating matrices, these similarities between user/items in this reduced representation are more robust because the new low-dimensional representation is fully specified (and faster!). It can be computed, using the simple cosine or dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Since this code is quite fast, lets work with the 1M dataset (results will be nicer!)\n",
    "data = pd.io.parsers.read_csv('ml-1m/ratings.dat', \n",
    "    names=['user_id', 'movie_id', 'rating', 'time'],\n",
    "    engine='python', delimiter='::')\n",
    "movies = pd.io.parsers.read_csv('ml-1m/movies.dat',\n",
    "    names=['movie_id', 'title', 'genre'],\n",
    "    engine='python', delimiter='::')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Lets create a rating Matrix of size M x N\n",
    "num_movies = np.max(data.movie_id.values)\n",
    "num_users = np.max(data.user_id.values)\n",
    "\n",
    "ratings_mat = np.ndarray(shape=(num_movies, \n",
    "                                num_users),\n",
    "                         dtype=np.uint8)\n",
    "ratings_mat[data.movie_id.values-1, data.user_id.values-1] = data.rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#Normalise matrix (subtract mean off)\n",
    "normalised_mat = ratings_mat - np.asarray([(np.mean(ratings_mat, 1))]).T\n",
    "A = normalised_mat.T /np.sqrt(num_movies -1)\n",
    "U, S, V = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Let's check the shape of the matices\n",
    "print(U.shape, S.shape,V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate cosine similarity, sort by most similar and return the top N.\n",
    "def top_cosine_similarity(data, movie_id, top_n=10):\n",
    "    index = movie_id - 1 # Movie id starts from 1\n",
    "    movie_row = data[index, :]\n",
    "    magnitude = np.sqrt(np.einsum('ij, ij -> i', data, data))\n",
    "    similarity = np.dot(movie_row, data.T) / (magnitude[index] * magnitude)\n",
    "    sort_indexes = np.argsort(-similarity)\n",
    "    return sort_indexes[:top_n]\n",
    "\n",
    "# Helper function to print top N similar movies\n",
    "def print_similar_movies(movie_data, movie_id, top_indexes):\n",
    "    print('Recommendations for {0}: \\n'.format(\n",
    "    movie_data[movie_data.movie_id == movie_id].title.values[0]))\n",
    "    for id in top_indexes + 1:\n",
    "        print(movie_data[movie_data.movie_id == id].title.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k = 50\n",
    "movie_id = 1 # Let's check for one movie\n",
    "top_n = 5\n",
    "\n",
    "sliced = V.T[:, :k] # representative data\n",
    "indexes = top_cosine_similarity(sliced, movie_id, top_n)\n",
    "print_similar_movies(movies, movie_id, indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**EXERCISE: Create a new method recommender method using the SVD.** </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "class SVD_CollaborativeFiltering:\n",
    "    \"\"\" Collaborative filtering using a custom sim(u,u'). \"\"\"\n",
    "    \n",
    "    def __init__(self,DataFrame, similarity=SimPearson,num_components=10):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "    def learn(self):\n",
    "        \"\"\" Prepare data structures for estimation. Similarity matrix for users \"\"\"\n",
    "                \n",
    "    def estimate(self, user_id, movie_id):\n",
    "        return 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "reco = SVD_CollaborativeFiltering(dataSmall_train,num_components=80)\n",
    "reco.learn()\n",
    "reco.estimate(user_id=2,movie_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.estimate,dataSmall_train,dataSmall_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's think about the problem! \n",
    "![alt Netflix vs Spotify](images/NetFlixVsSpotify.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Challenges on Recommender Music:\n",
    "* Cold Start Problem\n",
    " * Really sparse problem\n",
    " * Cross domain techniques\n",
    "* Automatic PlayList Generation\n",
    " * Contextual features are really important: from time, mode or location to weather\n",
    "* Evaluation\n",
    " * How do we know that was a good track for the user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RecSys 2018\n",
    "Interesting Challenge on Recommender Systms:\n",
    "http://www.recsyschallenge.com/2018/\n",
    "\n",
    "This challenge, in collaboration with Spotify, focuses on music recommendation, specifically the challenge of automatic playlist continuation. By suggesting appropriate songs to add to a playlist, a Recommender System can increase user engagement by making playlist creation easier, as well as extending listening beyond the end of existing playlists.\n",
    "\n",
    "Spotify have released a public dataset of playlists, consisting of a large number of playlist titles and associated track listings. For the evaluation purpose, there is a set of playlists from which a number of tracks have been withheld. The task will be to predict the missing tracks in those playlists.\n",
    "\n",
    "\n",
    "![alt recsys](images/recsys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;font-size:16px;\">\n",
    "<h3>Evaluating Recommender systems.</h3>\n",
    "</div>\n",
    "\n",
    "\n",
    "Can we evaluate if  our recommender is <b>good</b>?\n",
    "\n",
    "If we look at this image: \n",
    "![alt topAmazon](images/topAmazon.png)\n",
    "One may end with the following questions:\n",
    "\n",
    "1st Question: <b> Which prodcuts should be in this list?</b>\n",
    "\n",
    "2nd Question: <b> How do we know that these are good recommendations?</b>\n",
    "\n",
    "3rd Question: <b> Good in terms of what</b>\n",
    "* Data Scientist vs. Marketing vs. Economist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "What does <b>Good</b> means??\n",
    " * Recommendation accuracy?\n",
    " * Recommendation quality?\n",
    " * System usability?\n",
    " * System  satisfaction?\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### There is three main ways to evaluate a Recommender Systems\n",
    "The evaluation of the recommender systems is one the <b>most critical</b> steps when building a recommender system.\n",
    "* A single criterion cannot capture many goals of the designer.\n",
    "\n",
    "1) Offline Evaluation\n",
    " + Historical, such as ratings, are used. \n",
    " + In some cases, temporal information is also provided with the ratings, such as the time-stamp at which the information was obtained.\n",
    "\n",
    "2) User Studies\n",
    " + Test subjects are actively recruited, and asked to interact\n",
    " + with the recommendation system to perform some actions. Example: satisfaction questionaries\n",
    "\n",
    "\n",
    "3) Online Evaluation\n",
    " + Online evaluation also leverage user studies except that users are real users of fully developed  or commercial  system. \n",
    " + The user directly plays with the system,  usually different methods are compared with different random uses\n",
    " \n",
    "![alt AB-testing](images/ab-testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There exist several ways to evaluate a recommender system. Ones of the most populars are RMSE and MAE. However, <b>these metrics do not really measure the user experience</b>. \n",
    "\n",
    "Top-N performance metric can be also used and in fact, the evaluation with this metric is more closer to what is important for the user, just those top recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**EXERCISE:**\n",
    "Create a new method in the recommendetion class that returns the TOP N recomendations for a user.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def getTopMovies(reco,user_id,N = 10):\n",
    "    # Returns the N best (and not seen) movies for the user_id. \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to compute the precision-recall?\n",
    "We can use the strategy followed by P. Cremonesi et.al. in http://dl.acm.org/citation.cfm?id=1864721 <br>\n",
    "In order to measure the precision recall, first the models is trained using the training data, and then, for each item $i$ rated with 5 stars in the test data set:\n",
    "* A set of 100 random unseen movies for the user of the item $i$ are seleted. We assume that these random movies will not be at the same interest than the 5 star movie\n",
    "* We predict the rating of the movie of item $i$ and 100 random unseen movies.\n",
    "* We form a rank list by ordering all the 101 item according to the predicted rating. Let denote $p$ the rank of the test item $i$ within the list. The best results correspondes to the case the test item $i$ precedes all the random items (i.e., p=1).\n",
    "* A top-N recommendation list by piking the N top ranked items from the list. If $p \\leq N$ we have a hit. Otherwise we have a miss. Chanches of hit incresases as N is higher.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br>\n",
    "<div class=\"alert alert-success\">\n",
    "**EXERCISE:**\n",
    "Create a new method for the evaluation of the precision/recall curve using Top-N recomendations output.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluateTop(estimate_f,data_train,data_test, N = 10):\n",
    "    \"\"\" Precision-Recall evaluation bas on top Recommendations \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "precision,recall = evaluateTop(reco.estimate,dataSmall_train,dataSmall_test,N=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot( range(0,len(recall)),recall)\n",
    "plt.plot( range(0,len(recall1)),recall1)\n",
    "plt.plot( range(0,len(recall2)),recall2)\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('recall')\n",
    "plt.show()\n",
    "plt.plot(recall, precision)\n",
    "plt.plot(recall1, precision1)\n",
    "plt.plot(recall2, precision2)\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;\">\n",
    "<h3>Other Popular Methods</h3>\n",
    "</div>\n",
    "\n",
    "### Restricted Boltzmann Machines for Collaborative Filtering  (2007)\n",
    "http://www.cs.toronto.edu/~fritz/absps/netflix.pdf\n",
    "\n",
    "### Method: SLIM - Sparse Linear Methods for Tope-N recommender systems (2011)\n",
    "\n",
    "Computes the item-item relations, by estimating an $item \\times item$ sparse aggregation coefficient matrix R.\n",
    "The recommendation score of an unrated item $i$ for a user $u$ is:\n",
    "$$ \\hat{r}_{u,i} = r^T_u s_i$$\n",
    "and the minimization problem is defined as:\n",
    "$$\\underset{S}{\\text{minimize}} \\frac{1}{2} \\sum_{u,i}(r_{u,i} - \\hat{r}_{u,i}^2) + \\frac{\\beta}{2}||S||^2_F  + \\lambda||S||_1$$\n",
    "$$ \\text{subject to  }  R\\geq 0, \\text{and } Diag(S) = 0$$\n",
    "![alt slim](images/slim.png)\n",
    "\n",
    "http://ieeexplore.ieee.org/document/6137254/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
