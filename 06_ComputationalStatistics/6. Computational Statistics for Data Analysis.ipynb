{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> \n",
    "# Computational Statistics for Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-error\"><strong><center><small>Statistics is the discipline of using data samples to support claims about populations.<small></center></strong> </div>\n",
    "\n",
    "# Statistics is based on two main concepts: #\n",
    "\n",
    "* A **population** is a collection of objects, items (‚Äúunits‚Äù) about which information is sought.\n",
    "\n",
    "* A **sample** is a part of the population that is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1 Descriptive Statistics.\n",
    "* 1.1 Getting data\n",
    "* 1.2 Data preparation\n",
    "* 1.3 Improving data as a pandas DataFrame\n",
    "* 1.4 Data cleaning and preparation\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Exploratory Data Analysis.\n",
    "* 2.1 Summarizing the data: mean, variance, median, quantiles & percentiles\n",
    "* 2.2 Histogram\n",
    "* 2.3 Data distributions\n",
    "    * 2.3.1 PMF\n",
    "    * 2.3.2 CDF\n",
    "* 2.4 Outliers\n",
    "* 2.5 Measuring asymmetry\n",
    "    * 2.5.1 Skewness\n",
    "    * 2.5.2 Pearson's median skewness coefficient\n",
    "* 2.6 Relative risk\n",
    "* 2.7 A firts glimpse to Conditional Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Probabilities. \n",
    "* 3.1 Probability rules\n",
    "* 3.2 Binomial distribution\n",
    "* 3.3 Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"images/taft_puck_eggdig.jpg\">\n",
    "</center><center><img src=\"images/eggs.png\"></center>\n",
    "<center><small>Headline from Chicago Tribune June 13, 1897.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Do first babies arrive late?\"\n",
    "\n",
    "***From [Think Stats: Probability and Statistics for Programmers](http://www.greenteapress.com/thinkstats/), by Allen B. Downey, published by O'Reilly Media.***\n",
    "\n",
    "Some people believe it is true, but **without data analysis** to support it, this claim is a case of *anecdotal evidence*:\n",
    "\n",
    "* There are a *small number of samples* (personal experience, friends, etc.).\n",
    "* There is a *selection bias*: most *believers* are interested in this claim because their first babies were late.\n",
    "* There is a *confirmation bias*: believers might be more likely to contribute data that confirm it.\n",
    "* Sources are *innaccurate*: personal stories are subject to memory deformations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "# 1 Descriptive Statistics.\n",
    "* 1.1 Getting data\n",
    "* 1.2 Data preparation\n",
    "* 1.3 Improving data as a pandas DataFrame\n",
    "* 1.4 Data cleaning \n",
    " <font\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Getting Data\n",
    "\n",
    "There is an interesting and publicly available **data source** to check the claim of babies arrival. Since 1973 the U.S. Centers for Disease Control and Prevention (CDC) have conducted a survey, the National Survey of Family Growth (NSFG), to gather *information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men's and women's health.* \n",
    "\n",
    "Data can be downloaded from: \n",
    "\n",
    "http://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm#cyc6downdatafiles\n",
    "\n",
    "We will use this file: \n",
    "\n",
    "* Female Pregnancy Data File (2002FemPreg.dat): one record for each pregnancy reported by a respondent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13593 pregnancies in our data. The meaning of the data is (each record is in a line):\n",
    "\n",
    "+ <code>case.id</code> is the ID of the respondent. From col 1 to 12.\n",
    "+ <code>prg.lenght</code> is the duration of the pregnancy in weeks. From col 275 to 276. \n",
    "+ <code>outcome</code> is the outcome of the pregnancy (1 = live birth). Col 277.\n",
    "+ <code>birth.ord</code> is the integer birth order of each live birth. From col 278 to 279.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small> If curious: Online documentation of the survey is at http://www.icpsr.umich.edu/nsfg6.<small> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data preparation\n",
    "\n",
    "One of the reasons we are using a general-purpose language such as Python rather than a stats language like R is that for many projects the *hard* part is preparing the data, not doing the analysis.\n",
    "\n",
    "The most common steps are:\n",
    "\n",
    "1. **Getting the data**. Data can be directly read from a file or it might be necessary to scrap the web.\n",
    "2. **Parsing the data**.  Of course, this depends on what format it is in: plain text, fixed columns, CSV, XML, HTML, etc.\n",
    "3. **Cleaning the data**.  Survey responses and other data files are almost always incomplete.  Sometimes there are multiple codes for things like, *not asked*, *did not know*, and *declined to answer*. And there are almost always errors. A simple strategy is to remove or ignore incomplete records.\n",
    "4. **Building data structures**. Once you read the data, you usually want to store it in a data structure that lends itself to the analysis you want to do.\n",
    "\n",
    "If the data fits into memory, building a data structure is usually the way to go.   If not, you could build a **database**, which is an out-of-memory data structure. Most databases provide a mapping from keys to values, so they are like dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('files/2002FemPreg.dat', 'r')\n",
    "\n",
    "# Let's build a list of lists.\n",
    "\n",
    "preg=[]\n",
    "for line in file:\n",
    "    preg.append([int(line[:12]), int(line[274:276]), int(line[276]),\n",
    "                 int(line[277:279])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops! There is something wrong in the data file!\n",
    "\n",
    "By inspecting the data we can observe that there are some empty records that caused an error to the ``int`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('files/2002FemPreg.dat', 'r')\n",
    "\n",
    "def chr_int(a):\n",
    "    if a == '  ':\n",
    "        return 0\n",
    "    else:\n",
    "        return int(a)\n",
    "        \n",
    "preg=[]\n",
    "for line in file:\n",
    "    lst  = [int(line[:12]), int(line[274:276]), int(line[276]),\n",
    "                 chr_int(line[277:279])]\n",
    "    preg.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First record: ', preg[0])\n",
    "print('The number of entries is: ', len(preg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Importing data as a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(preg) #  Two-dimensional size-mutable, potentially heterogeneous tabular \n",
    "        #data structure with labeled axes \n",
    "    \n",
    "df.columns = ['caseId', 'prgLength', 'outcome', 'birthOrd']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby('birthOrd').size()\n",
    "print('The number of first babies is: ',counts[1])\n",
    "print('Number of babies according to their order: ',counts) \n",
    "\n",
    "# also: df.outcome.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of  births according to the order.\n",
    "\n",
    "Let's build a partition of *live births* into two groups: first babies and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide records into two lists: first babies and others.\n",
    "\n",
    "firstbirth = df[ (df.outcome == 1) & (df.birthOrd == 1)]\n",
    "print('The number of first babies is: ', firstbirth.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "othersbirth = df[(df.outcome == 1) & (df.birthOrd >= 2)]\n",
    "print('The number of others babies is: ', othersbirth.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Cleaning\n",
    "\n",
    "The most common steps are:\n",
    "\n",
    "+ **Sample the data**. If the amount of raw data is huge, processing all of them may require an extensive amount of processing power which may not be practical.  \n",
    "\n",
    "\n",
    "+ **Impute missing data**. It is quite common that some of the input records are incomplete in the sense that certain fields are missing or have input error.  Validate the format and value! \n",
    "\n",
    "In case of fields missing: \n",
    "<small>\n",
    "* (a) Discard the whole record if it is incomplete; \n",
    "* (b) Infer the missing value based on the data from other records (fill the missing data with the average or the median).\n",
    "<small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Normalize numeric value**. Normalize data is about transforming numeric data into a uniform range.\n",
    "+ **Reduce dimensionality**. High dimensionality can be a problem for some machine learning methods.  There are two ways to reduce the number of input features.  One is about *removing* *irrelevant* input variables, another one is about $removing$ $redundant$ input variables.\n",
    "+ **Add derived features**. In some cases, we may need to compute additional attributes from existing attributes (f.e. converting a geo-location to a zip code, or converting the age to an age group).\n",
    "+ **Discretize numeric value into categories**. Discretize data is about cutting a continuous value into ranges and assigning the numeric with the corresponding bucket of the range it falls on.  For numeric attribute, a common way to generalize it is to discretize it into ranges, which can be either constant width (variable height/frequency) or variable width (constant height).\n",
    "+ **Binarize categorical attributes**. Certain machine learning models only take binary input (or numeric input).  In this case, we need to convert categorical attribute into multiple binary attributes, while each binary attribute corresponds to a particular value of the category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Select, combine, aggregate data**. Designing the form of training data is the most important part of the whole predictive modeling exercise because the accuracy largely depends on whether the input features are structured in an appropriate form that provide strong signals to the learning algorithm. Rather than using the raw data as it is, it is quite common that multiple pieces of raw data need to be combined together, or aggregating multiple raw data records along some dimensions (e.g. clinical vs. demographic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> \n",
    "      \n",
    "## 2 Exploratory Data Analysis.\n",
    "* 2.1 Summariizing the data: mean, variance, median, quantiles & percentiles\n",
    "* 2.2 Histogram\n",
    "* 2.3 Data distributions\n",
    "* 2.3.1 PMF\n",
    "* 2.3.2 CDF\n",
    "* 2.4 Outliers\n",
    "* 2.5 Measuring asymmetry\n",
    "* 2.5.1 Skewness\n",
    "* 2.5.2 Pearson's median skewness coefficient\n",
    "* 2.6 Relative risk\n",
    "* 2.7 A firts glimpse to Conditional Probability\n",
    "<font\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Summarizing the data: \n",
    "#### 2.1.1 Sample Mean \n",
    "\n",
    "If you have a sample of $n$ values, $x_i$, the **sample mean** is the sum of the values divided by the number of values:\n",
    "\n",
    "$$ \\mu = \\frac{1}{n} \\sum_i x_i$$\n",
    "\n",
    "The **mean** is the most basic and important summary statistic. It describes the *central tendency* of a sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there is a difference between firstbirth and othersbirth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean of the first birth is: ', firstbirth['prgLength'].mean())\n",
    "print('The mean of the non first birth is: ', othersbirth['prgLength'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The difference in time is: ', abs(firstbirth['prgLength'].mean()-\n",
    "          othersbirth['prgLength'].mean()),\"weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The difference in time is: ',abs(firstbirth['prgLength'].mean()-\n",
    "          othersbirth['prgLength'].mean())*7,\"days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The difference in time is: ',abs(firstbirth['prgLength'].mean()-\n",
    "          othersbirth['prgLength'].mean())*7*24, \"hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference in sample means can be considered a first evidence of our hypothesis!\n",
    "\n",
    "\n",
    "**Comment: ** Do not confuse both concepts: the sample mean and the population mean.  The first one is the mean of samples taken from the population and the second one is the mean of the whole population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Sample Variance\n",
    "\n",
    "Usually, the mean is not a sufficient descriptor of the data, we can do a little better with two numbers: mean and **variance**:\n",
    "\n",
    "$$ \\sigma^2 = \\frac{1}{n} \\sum_i (x_i - \\mu)^2 $$\n",
    "\n",
    "**Variance** $\\sigma^2$ describes the *spread* of data. The term $(x_i - \\mu)$ is called the *deviation from the mean*, so variance is the mean squared deviation.\n",
    "\n",
    "The square root of variance, $\\sigma$, is called the **standard deviation**. We define standard deviation because variance is hard to interpret (in the case the units are grams, the variance is in grams squared).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = firstbirth['prgLength'].mean()\n",
    "mu2 = othersbirth['prgLength'].mean()\n",
    "var1 = firstbirth['prgLength'].var()\n",
    "var2 = othersbirth['prgLength'].var()\n",
    "std1 = firstbirth['prgLength'].std()\n",
    "std2 = othersbirth['prgLength'].std()\n",
    "print('Firstbirth mu1:', mu1, 'var1:', var1, 'std1:', std1)\n",
    "print('Othersbirhts mu2:', mu2, 'var2:', var2, 'std2:', std2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Sample Median\n",
    "\n",
    "The statistical median is an order statistic that gives the *middle* value of a sample. It is a value more robust to ouliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstbirthmedian= firstbirth['prgLength'].median()\n",
    "othersbirthmedian= othersbirth['prgLength'].median()\n",
    "print('The median measures of both first- and otherbirths are: ', \n",
    "      firstbirthmedian, othersbirthmedian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/mean_median.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Summarizing the data: Quantiles & Percentiles\n",
    "\n",
    "Order the sample $\\{ x_i \\}$, then find $x_p$ so that it divides the data into two parts where:\n",
    "\n",
    "+ a fraction $p$ of the data values are less than or equal to $x_p$ and\n",
    "+ the remaining fraction $(1 ‚àí p)$ are greater than $x_p$.\n",
    "\n",
    "That value $x_p$ is the pth-quantile, or 100√ópth percentile.\n",
    "\n",
    "**5-number summary**: $x_{min}, Q_1, Q_2, Q_3, x_{max}$, where $Q_1$ is the 25√ópth percentile,\n",
    "$Q_2$ is the 50√ópth percentile and $Q_3$ is the 75√ópth percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common representation of a distribution is a [**histogram**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html), which is a graph that shows the frequency of each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb=firstbirth['prgLength']\n",
    "\n",
    "fb.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "ob=othersbirth['prgLength']\n",
    "ob.hist(density=0, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "fb.hist(density=True, bins=30, alpha=.5)   # default number of bins = 10\n",
    "ob.hist(density=True, bins=30, alpha=.5, color=sns.desaturate(\"indianred\", .75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Computes several descriptive statistics:\n",
    "# size of the data \n",
    "# minimum and maximum value of data array\n",
    "#¬†arithmetic mean \n",
    "# unbiased variance \n",
    "# biased skewness \n",
    "# kurtosis (Fisher)\n",
    "\n",
    "print('Firstbirhts: ', stats.describe(fb.values))\n",
    "\n",
    "print('Othersbirths: ', stats.describe(ob.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Distributions\n",
    "\n",
    "Summarizing can be dangerous: very different data can be described by the same statistics. It must be validated by inspecting the data.\n",
    "\n",
    "We can look at the **data distribution**, which describes how often (frequency) each value appears.\n",
    "\n",
    "\n",
    "We can normalize the frequencies of the histogram by dividing/normalizing by $n$, the number of samples. The normalized histogram is called **Probability Mass Function (PMF)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, execute the command 'pip3 install seaborn'\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "fb.hist(density=True, alpha=.5)\n",
    "ob.hist(density=True, alpha=.5, color=sns.desaturate(\"indianred\", .75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cumulative distribution function (CDF)**, or just distribution function, describes the probability that a real-valued random variable X with a given probability distribution will be found to have a value less than or equal to x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.hist(density=1, cumulative=True, linewidth=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.hist(density=1,  cumulative=True, linewidth=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.hist(bins=10, density=1,  alpha=.5)   # default number of bins = 10\n",
    "ob.hist(bins=10, density=1,  alpha=.5, color=sns.desaturate(\"indianred\", .75))\n",
    "\n",
    "# Check with 20, 30, 60 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.hist(density=True, histtype='step', cumulative=True,  linewidth=3.5, bins=30)\n",
    "ob.hist(density=True, histtype='step', cumulative=True,  linewidth=3.5, bins=30, color=sns.desaturate(\"indianred\", .75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Outliers\n",
    "\n",
    "**Outliers** are data samples with a value that is far from the central tendency.\n",
    "\n",
    "We can find outliers by:\n",
    "\n",
    "+ Computing samples that are *far* from the median.\n",
    "+ Computing samples whose value *exceeds the mean* by 2 or 3 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.outcome == 1) & (df['prgLength'] < df['prgLength'].median() - 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.outcome == 1) & (df['prgLength'] > df['prgLength'].median() + 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think that outliers correspond to errors, an option is to trim the data by discarting the highest and lowest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(df.index[(df.outcome == 1) & \n",
    "                       (df['prgLength'] > df['prgLength'].median() + 6)])\n",
    "\n",
    "df3 = df2.drop(df2.index[(df2.outcome == 1) & \n",
    "                         (df2['prgLength'] < df2['prgLength'].median() - 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb3 = df3[(df3.outcome == 1) & (df3.birthOrd == 1)]\n",
    "\n",
    "mu3fb = fb3['prgLength'].mean()\n",
    "std3fb = fb3['prgLength'].std()\n",
    "md3fb = fb3['prgLength'].median()\n",
    "\n",
    "print('Before outliers removing: ', mu1, std1, firstbirthmedian, \n",
    "          firstbirth['prgLength'].min(), firstbirth['prgLength'].max())\n",
    "print('After outliers removing: ', mu3fb, std3fb, md3fb, \n",
    "          fb3['prgLength'].min(),fb3['prgLength'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob3 = df3[(df3.outcome == 1) & (df3.birthOrd >= 2)]\n",
    "\n",
    "mu3ob = ob3['prgLength'].mean()\n",
    "std3ob = ob3['prgLength'].std()\n",
    "md3ob = ob3['prgLength'].median()\n",
    "\n",
    "print('Before outliers removing: ', mu2, std2, othersbirthmedian, \n",
    "          othersbirth['prgLength'].min(),othersbirth['prgLength'].max())\n",
    "print('After outliers removing: ', mu3ob, std3ob, md3ob, \n",
    "          ob3['prgLength'].min(), ob3['prgLength'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "\n",
    "df3.prgLength[(df3.outcome == 1)].plot(alpha=.5, color='blue')\n",
    "df.prgLength[(df.outcome == 1)].plot(alpha=.5, \n",
    "                            color=sns.desaturate(\"indianred\", .95))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is happening near the mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "countfb,divisionfb = np.histogram(fb3['prgLength']) #what is the difference with hist()?!\n",
    "countob,divisionob = np.histogram(ob3['prgLength'])\n",
    "print (countfb-countob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "val = [(divisionfb[i]+divisionfb[i+1])/2 for i in range(len(divisionfb)-1)]\n",
    "plt.plot(val, countfb-countob, 'o-', label='difference') \n",
    "plt.plot(val, countfb, 'r*-', label='First babies') \n",
    "plt.plot(val, countob, 'g+-', label='Other babies') \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "val = [(divisionfb[i]+divisionfb[i+1])/2 for i in range(len(divisionfb)-1)]\n",
    "plt.plot(val, countfb-countob, 'o-', label='difference') \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is still some evidence for our hypothesis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Measuring asymmetry.\n",
    "\n",
    "**Skewness** is a statistic that measures the asymmetry of set of $n$ data samples $x_i$:\n",
    "\n",
    "$$ g_1 = \\frac{m_3}{s^3}= \\frac{\\frac{1}{n} \\sum_i (x_i - \\mu)^3 }{[\\frac{1}{n-1} \\sum_i (x_i - \\mu)^2]^{\\frac{3}{2}} } $$\n",
    "\n",
    "The  the numerator is the mean cubed deviation and the denominator  is the mean squared deviation (or variance) to power 3.\n",
    "\n",
    "Negative deviation indicates that the distribution \"skews left\" (it extends farther to the left than to the right).\n",
    "\n",
    "**Skewness** can be affected by outliers!!! A simpler alternative is to look at the relationship between mean ($\\mu$) and median ($\\mu_{\\frac{1}{2}}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Pearson's median skewness coefficient** is a more robust alternative:\n",
    "\n",
    "$$ g_p = \\frac{3(\\mu - \\mu_{\\frac{1}{2}})}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb33=fb3['prgLength']\n",
    "ob33=ob3['prgLength']\n",
    "\n",
    "print('Firstbirhts: ', stats.describe(fb33.values),'\\n')\n",
    "print('Otherbirhts: ', stats.describe(ob33.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Write a function to compute $g_1$ and $g_p$ of the pregnancy length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "+ Read the file ``run10.txt`` from the ``files`` directory. It represents 16.924 runners who finished the 2012 Cherry Blossom 10 mile run in USA. The file is a ``tab``separated file. It can be read with the pandas ``read_table`` function.\n",
    "+ Compute the mean time.\n",
    "+ Compute the difference in mean between men and women.\n",
    "+ Visualize both distributions (normalized histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**: \n",
    "\n",
    "+ Could you give a real example, where for all data samples, $x_i \\leq \\mu$? \n",
    "+ Could you give a real example, where for all data samples, $x_i \\leq \\mu_{\\frac{1}{2}}$? This is really a distribution that skews left!\n",
    "+ If we ask to a random group of people \"What is your position with respect to the average driver?\", what kind of distribution would we get? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Relative Risk\n",
    "\n",
    "Let's say that a baby is \"early\" if it is born during week 37 or earlier, \"on time\" if it is born during week 38, 39 or 40, and \"late\" if it is born during week 41 or later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's compute the probability of being *early*, *on time* and *late* for first babies and the others.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Firsts babies: \")\n",
    "print('Early: ',len(fb3[fb3['prgLength'] <38])/float(len(fb3.index)))\n",
    "\n",
    "print('Late: ', len(fb3[fb3['prgLength'] >40])/float(len(fb3.index)))\n",
    "\n",
    "print('On time: ',len(fb3[(fb3['prgLength'] >37)&(fb3['prgLength'] < 41)])/float(len(fb3.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Other babies:\")\n",
    "print(\"Early\", len(ob3[ob3['prgLength'] <38])/float(len(ob3.index)))\n",
    "\n",
    "print(\"Late\", len(ob3[ob3['prgLength'] >40])/float(len(ob3.index)))\n",
    "\n",
    "print(\"On time\", len(ob3[(ob3['prgLength'] >37) &\n",
    "                        (ob3['prgLength']<41)])/float(len(ob3.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **relative risk** is the ratio of two probabilities. In our case, the probability that a first baby is born early is 17%. For other babies is 16%, so the relative risk is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyfb = len(fb3[fb3['prgLength'] <38])/float(len(fb3.index))\n",
    "earlyob = len(ob3[ob3['prgLength'] <38])/float(len(ob3.index))\n",
    "print(\"First babies are about\", earlyfb/earlyob, \"more likely to be early.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that first babies are about 7% more likely to be early. For the case of late births:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latefb = len(fb3[fb3['prgLength'] >40])/float(len(fb3))\n",
    "lateob = len(ob3[ob3['prgLength'] >40])/float(len(ob3))\n",
    "print(\"First babies are about\", latefb/lateob, \"more likely to be late.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that first babies are about 67% more likely to be late. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 A firts glimpse to Conditional Probability\n",
    "\n",
    "Imagine that someone you know is pregnant and it is the beginning of week 39. What is the chance that the baby will be born in the week 39? What is the chance if it is a first baby?\n",
    "\n",
    "We can ask these questions by computing a **conditional probability**, $P(X|Y)$.\n",
    "\n",
    "In our first question, the event X is a birth in week 39 and the event Y is that we know that the baby didn't arrive during weeks 0-38. In the second question, we also know that it is a first baby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to compute these chances is to drop from our data the cases that do not fulfill the conditions and then renormalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.drop(df3.index[df3['prgLength'] < 39]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compute the probability that the baby will be born in the week 39 for a pregnant woman in the beginning of week 39."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df4[(df4.prgLength == 39)].index)/float(len(df4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add the second condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb39 = df4[(df4.birthOrd == 1)]\n",
    "ob39 = df4[(df4.birthOrd > 1)]\n",
    "\n",
    "fb39['prgLength'].hist(bins=6,  density=True, alpha=.5)   # default number of bins = 10, blue\n",
    "ob39['prgLength'].hist(bins=6,  density=True, alpha=.5, color=sns.desaturate(\"indianred\", .75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability First baby to be born on week 39:', \n",
    "    len(fb39[(fb39.prgLength == 39)].index)/\n",
    "    float(len(fb39.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability non first baby to be born on week 39:',\n",
    "    len(ob39[(ob39.prgLength == 39)].index)/\n",
    "    float(len(ob39.index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussions.\n",
    "\n",
    "After exploring the data we have seen some **appearent effects** that seem to support our first hypothesis:\n",
    "\n",
    "+ **Data description**: The mean pregnant lenght for first babies is 38.76 and for other babies is 38.65.\n",
    "\n",
    "+ **Relative risk**: First babies are about 67% more likely to be late.\n",
    "\n",
    "+ **Conditional probability**: If someone is pregnant and it is the beginning of week 39, the chance (63% vs. 72%) that the baby will be born in the week 39 is lower if it is the first baby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercises: Other possible experiments\n",
    "\n",
    "We can compare the first and others for the same woman. While may be unlikely it could still be that a tendency exists for a woman's second, third, etc, child comes earlier.\n",
    "\n",
    "<small>(Result:  The second baby is born about some hours earlier, but this difference is not *statistically significant*.)<small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## 3 Probabilities. \n",
    "* 3.1 Probability rules\n",
    "* 3.2 Binomial distribution\n",
    "* 3.3 Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common definition of **probability** is a *frequency expressed as a fraction* of the universe of possible outcomes. \n",
    "\n",
    "   -Thus, it is a real value between 0 and 1 that is intended to be a measure corresponding to the idea that some things are more likely than others.\n",
    "\n",
    "**Frequentism** because it defines probability in terms of frequencies.\n",
    "\n",
    "An alternative is **Bayesianism**, which defines probability as a degree of belief that an event will occur.\n",
    "\n",
    "   -Example: What is the probability that Thaksin Shinawatra is the Prime Minister of Thailand?\n",
    "\n",
    "The *things* we assign probabilities are called **events**, E.  A *situation* where E might or might not happen is called a **trial**.\n",
    "\n",
    "   -In the case of a six-sided die, each roll is called a **trial**. If we want to compute P(6), each time a 6 appears is called a **success**. Other trials are called **failures**. \n",
    "\n",
    "If in a *finite series of n identical trials* we observe s successes, the **probability of the success** is s/n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Probability Rules \n",
    "\n",
    "\n",
    "- A rule that is true when A and B are **not independent**: \n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\mbox{ and } B)}{P(B)}$$\n",
    "    \n",
    "    - From that we can derive: \n",
    "\n",
    "$$P(A \\mbox{ and } B) = P(A) P(B|A) = P(B) P(A|B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A rule that is true, when A and B are **independent**: $$P(A \\mbox{ and } B) = P(A) P(B)$$. \n",
    "\n",
    "\n",
    "A and B are **independent** if the fact that A occurred, does not change the probability of B and viceversa. Trials corresponding to tossing a coin are independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**: \n",
    "+ If I have two children, what is the probability to have 2 girls?\n",
    "+ If I have two children and we know that at least one of them is a girl, what is the probability that they are two girls?\n",
    "+ If I have two children and we know that the older one is a girl, what is the probability that they are two girls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More probability rules\n",
    "\n",
    "We say that two events are **mutually exclusive** if:\n",
    "\n",
    "$$ P(A | B) = P(B | A) = 0 $$\n",
    "\n",
    "In this case it is easy to show that:\n",
    "\n",
    "$$ P(A \\mbox{ or } B) = P(A) + P(B)$$\n",
    "\n",
    "If A and B are not mutually exclusive:\n",
    "\n",
    "$$ P(A \\mbox{ or } B) = P(A) + P(B) - P(A \\mbox{ and } B) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**: Counting is the most basic skill to solve probability problems.\n",
    "\n",
    "+ Q: For example, if I flip two coins, the chance of getting at least one tail is: 1/2 + 1/2?!\n",
    "+ Q: If I roll two dices and the total is 8, what is the probability that one of the dice is 6?\n",
    "+ Q: If I roll 100 dice, what is the probability of getting all sixes? \n",
    "+ Q: What is the probability of getting no sixes?\n",
    "+ Q: What is the probability of getting at least one six?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Binomial distribution\n",
    "\n",
    "More generally, the probability distribution that represents the probability of getting k times a success with probability p in n trials is:\n",
    "\n",
    "$$ PMF(k) = {n \\choose k} p^k (1-p)^{(n-k)}$$\n",
    "\n",
    "where ${n \\choose k} = \\frac{n!}{k!(n-k)!}$. This is called **binomial distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of having 2 successes in 5 trials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as sc\n",
    "n = 5\n",
    "k = 2\n",
    "print(sc.comb(n, k, exact=True)) # Chances of 2 successes in 5 trials\n",
    "p=sc.comb(n, k, exact=True)*1/2.0**2*(1-1/2.0)**(5-2)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of having 5 heads in 9 trials?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chances of 5 heads in 9 tosses\n",
    "\n",
    "a = sc.comb(9, 5, exact=True)\n",
    "print('The combinations of 9 on 5 are: ', a)\n",
    "\n",
    "p = 0.5\n",
    "print('Prob: ', a * p**5 * (1-p)**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What is the probability of having 6 sixes in 9 trials?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Äã\n",
    "\n",
    "## 3.3 Monte Carlo Experiments\n",
    "\n",
    "‚Äã\n",
    "\n",
    "**Monte Carlo experiments** are a broad class of computational algorithms that rely on *repeated random sampling to obtain numerical results*. Typically, one runs simulations many times over and over in order to obtain the distribution of an unknown probabilistic entity. (*Source: Wikipedia*)\n",
    "\n",
    "‚Äã\n",
    "\n",
    "**Trivial case**: What are the chances of getting a six in one trial?\n",
    "\n",
    "‚Äã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "N = 1000000 # perform N experiments\n",
    "M = 0 # number of times, we got 6\n",
    "\n",
    "for i in range(N):\n",
    "    outcome = random.randint(1, 6)\n",
    "    if outcome == 6:\n",
    "        M += 1\n",
    "\n",
    "Prob=M/float(N)\n",
    "print('I got six %d times out of %d' % (M, N), '; Prob = ', \n",
    "      Prob, 'Note that: 1/6=', 1/6.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the chances of getting a six in two trials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  chances of (exactly) 1 six in 2 trials\n",
    "\n",
    "a = sc.comb(2, 1, exact=True)\n",
    "p = 1/6.0\n",
    "\n",
    "print('Prob: ', a * p * (1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000 # perform N experiments\n",
    "M = 0 # no of times we get one 6\n",
    "\n",
    "for i in range(N):\n",
    "    outcome1 = random.randint(1, 6)\n",
    "    outcome2 = random.randint(1, 6)\n",
    "    if (outcome1 == 6 and outcome2 !=6) or (outcome1 != 6 and outcome2 == 6):\n",
    "        M += 1\n",
    "\n",
    "print('I got one six %d times out of %d' % (M, N), \n",
    "      '; Prob = ', float(M)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: You throw two dice, one black and one red. What is the probability that the number of eyes on the black die is larger than the number of eyes on the red die?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000 # perform N experiments\n",
    "M = 0 # no of times we get one 6\n",
    "\n",
    "for i in range(N):\n",
    "    outcome1 = random.randint(1, 6)\n",
    "    outcome2 = random.randint(1, 6)\n",
    "    if (outcome1 > outcome2):\n",
    "        M += 1\n",
    "        \n",
    "print('I got one six %d times out of %d' % (M, N), \n",
    "      '; Prob = ', float(M)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A more interesting case:** If I roll a dice 100 times, what is the chance of getting at least 6 sixes in a row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000 # perform N experiments\n",
    "M = 0 # no of times we get one 6\n",
    "T = 100\n",
    "success='666666'\n",
    "\n",
    "for i in range(N):\n",
    "    outcome=''\n",
    "    for i in range(T):\n",
    "\n",
    "        outcome=outcome+str(random.randint(1,6))\n",
    "\n",
    "    if (success in outcome):\n",
    "            M += 1\n",
    "\n",
    "print('I got 6 sixes in a row %d times out of %d' % (M, N), \n",
    "\n",
    "      '; Prob = ', float(M)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: What is the probability that Messi scores at least 1 goal in a row of 10 matches during a season? (Let's suppose that each match is an independent trial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**: Messi scores 0.83 goals per match (323 goals in 387 matches) and CR4 scores 0.62 (329 goals in 527 matches) goals per match.\n",
    "\n",
    "There are 42 matches in a season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>(Source: https://es.answers.yahoo.com/question/index?qid=20130928103148AAFQHsC)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Continous distributions\n",
    "\n",
    "So far, we have built **empirical distributions** (which represent the distributions of values in a sample), based on observations, but many real problems are well approximated by fitting **continous cumulative distributions functions (CDF)**. \n",
    "\n",
    "They are called in this way because the distribution is described by an analytic continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 The exponential distribution\n",
    "\n",
    "The CDF of the exponential distribution is:\n",
    "\n",
    "$$ CDF(x) = 1 -  \\exp^{- \\lambda x}$$ \n",
    "\n",
    "And its PDF is:\n",
    "\n",
    "$$ PDF(x) = \\lambda \\exp^{- \\lambda x}$$\n",
    "\n",
    "The parameter $\\lambda$ determines the shape of the distribution, the mean of the distribution is $1/\\lambda$ and its variance is $1/\\lambda^2$. The median is $ln(2)/\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real applications, exponential distributions appear when we have a series of events and estimate the events times, called interarrival times. \n",
    "\n",
    "**When the events are equally likely to occur at any time, the interarrival times distribution used to get exponential distribution.**\n",
    "\n",
    "As an example, the following figure shows the CDF of the interarrival times of birth in an Australian hospital. 44 births were registered in 24 hours, so the rate is  ùúÜ=0.0306  births/minute. The mean of the exponential distribution is  1/ùúÜ , so the mean time between births is 32.7 minutes.\n",
    "\n",
    "<center><img src='images/interarrivaltimes.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "l = 2\n",
    "x=np.arange(0,2.5,0.1)\n",
    "y= 1 - np.exp(-l*x)\n",
    "\n",
    "plt.plot(x,y,'-')\n",
    "plt.title('Exponential CDF: $\\lambda$ =%.2f' % l ,fontsize=15)\n",
    "plt.xlabel('x',fontsize=15)\n",
    "plt.ylabel('CDF',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "l = 2\n",
    "x=np.arange(0,2.5,0.1)\n",
    "y= l * np.exp(-l*x)\n",
    "plt.plot(x,y,'-')\n",
    "plt.title('Exponential PDF: $\\lambda$ =%.2f' % l, fontsize=15)\n",
    "plt.xlabel('x', fontsize=15)\n",
    "plt.ylabel('PDF', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurrence of Events\n",
    "\n",
    "The exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\n",
    "\n",
    "In real-world scenarios, the assumption of a constant rate (or probability per unit time) is rarely satisfied. For example, the rate of incoming phone calls differs according to the time of day. But if we focus on a time interval during which the rate is roughly constant, such as from 2 to 4 p.m. during work days, the exponential distribution can be used as a good approximate model for the time until the next phone call arrives. Similar caveats apply to the following examples which yield approximately exponentially distributed variables:\n",
    "\n",
    "- The time until a radioactive particle decays, or the time between clicks of a Geiger counter.\n",
    "- The time it takes before your next telephone call\n",
    "- The time until default (on payment to company debt holders) in reduced form credit risk modeling\n",
    "\n",
    "The random variable $X$ of the lifelengths of some batteries is associated with a probability density function of the form:\n",
    "\n",
    "$$ PDF(x) = \\frac{1}{4} \\exp^{- \\frac{x}{4}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.25\n",
    "x=np.arange(0,25,0.1)\n",
    "y= l * np.exp(-l*x)\n",
    "plt.plot(x,y,'-')\n",
    "plt.title('Exponential: $\\lambda$ =%.2f' % l ,fontsize=15)\n",
    "plt.xlabel('x',fontsize=15)\n",
    "plt.ylabel('PDF',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "The number of days ahead travelers purchase their airline tickets can be modeled by an exponential distribution with the average amount of time equal to 15 days. \n",
    "\n",
    "- (a) Find the probability that a traveler will purchase a ticket fewer than 10 days in advance. \n",
    "\n",
    "\n",
    "Answer: \n",
    "\n",
    "Remember: The parameter  ùúÜ  determines the shape of the distribution, the mean of the distribution is  1/ùúÜ. The median is  ùëôùëõ(2)/ùúÜ.\n",
    "\n",
    "First we need to calculate the rate parameter: ùúÜ=1/15=0.066667. Then we can calculate ùëÉ(ùëã<10)=1‚àíùëí(‚àíùúÜ*10)=0.4865.\n",
    "\n",
    "- (b) How many days do half of all travelers wait?\n",
    "\n",
    "Answer:  \n",
    "We know that the Median of the exponential distribution is ùëÄùëíùëëùëñùëéùëõ=(ln2)/ùúÜ=10.397"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "On the average, a certain device lasts 10 years. Assume that the length of time is exponentially distributed.\n",
    "\n",
    "- a) What is the probability that the device lasts more than 7 years?\n",
    "\n",
    "Answer: For the beginning ùúÜ=1/10=0.1\n",
    " ùëÉ(ùëã>7)=1‚àíùëÉ(ùëã‚â§7)=1‚Äì(1‚àíùëí‚àíùúÜùë•)=ùëí‚àíùúÜùë•=ùëí‚àí(0.1)(7)=0.4966\n",
    "\n",
    "- b) On the average, how long would five devices last if they are used one after another?\n",
    "\n",
    "Answer: On the average, 1 device lasts ten years. Therefore, 5 devices, if they are used one right after the other would last, on the average, (5)(10) = 50 years.\n",
    "\n",
    "- c) What is the probability that a device lasts between 9 and 11 years?\n",
    "\n",
    "Answer: ùëÉ(9<ùëã<11)=ùëÉ(ùëã<11)‚àíùëÉ(ùëã<9)=(1‚àíùëí‚àí(0.1)(11))‚Äì(1‚àíùëí‚àí(0.1)(9))=0.0737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 The normal distribution\n",
    "\n",
    "The **normal or Gaussian distribution** is the most used one because it describes a lot of phenomena and because it is amenable for analysis. \n",
    "\n",
    "Its CDF has no closed-form expression and its more common representation is the PDF:\n",
    "\n",
    "$$ PDF(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(x-\\mu)^2}{2 \\sigma^2} \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=5 # mean\n",
    "s=1 # standard deviation\n",
    "x=np.arange(0,15,0.1)\n",
    "y=(1/(np.sqrt(2*np.pi*s*s)))*np.exp(-(((x-u)**2)/(2*s*s)))\n",
    "plt.plot(x,y,'-')\n",
    "plt.title('Gaussian PDF: $\\mu$=%.1f, $\\sigma$=%.1f' % (u,s),fontsize=15)\n",
    "plt.xlabel('x',fontsize=15)\n",
    "plt.ylabel('Probability density',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "    * Measures of size of living tissue (length, height, skin area, weight);\n",
    "    * The length of inert appendages (hair, claws, nails, teeth) of biological specimens, in the direction of growth; presumably the thickness of tree bark also falls under this category;\n",
    "    * Certain physiological measurements, such as blood pressure of adult humans.\n",
    "\n",
    "\n",
    "There exist much more distributions as: \n",
    "- the Pareto distribution (describing e.g. the distribution of wealth, cities sizes, sand particles, forest fires and earthquakes,\n",
    "- the lognormal distribution (describing the adult weights), etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Central Limit Theorem\n",
    "\n",
    "The normal distribution is also important, because it is involved in the Central Limit Theorem:\n",
    "\n",
    "> Take the mean of $n$ random samples from ANY arbitrary distribution with a $well$ $defined$ standard deviation $\\sigma$ and mean $\\mu$. As $n$ gets bigger the **distribution of the sample mean** will always converge to a Gaussian (normal) distribution with mean $\\mu$ and standard deviation $\\frac{\\sigma}{\\sqrt{n}}$.\n",
    "\n",
    "Colloquially speaking, the theorem states the distribution of an average tends to be normal, even when the distribution from which the average is computed is decidedly non-normal. This explains the ubiquity of the Gaussian distribution in science and statistics. \n",
    "\n",
    "#### Example: Uniform Distribution\n",
    "\n",
    "The uniform distribution is obviously non-normal.  Let's call it the $parent$ $distribution$.\n",
    "\n",
    "To compute an average, two samples are drawn ($n=2$), at random, from the parent distribution and averaged. Then another sample of two is drawn and another value of the average computed.  This process is repeated, over and over, and averages of two are computed.  \n",
    "\n",
    "Repeatedly taking more elements ($n = 3,4...$) from the parent distribution, and computing the averages, produces a normal probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, sharey=True, squeeze=True, figsize=(11, 5))\n",
    "x = np.linspace(0, 1, 100)\n",
    "for i in range(4):\n",
    "    f = np.mean(np.random.random((10000, i+1)), 1)\n",
    "    m, s = np.mean(f), np.std(f, ddof=1)\n",
    "    fn = (1/(s*np.sqrt(2*np.pi)))*np.exp(-(x-m)**2/(2*s**2))  # normal pdf            \n",
    "    ax[i].hist(f, 40, density=True, color=[0, 0.2, .8, .6]) \n",
    "    ax[i].set_title('n=%d' %(i+1))\n",
    "    ax[i].plot(x, fn, color=[1, 0, 0, .6], linewidth=10)\n",
    "    \n",
    "plt.suptitle('Demonstration of the central limit theorem for a uniform distribution', y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Central Limit Theorem explains the importance of normal distributions in the real world. Many features and properties of the living beings depend on genetic and environmental factors which effect usually is additive.** The measured features are sum of manny small effects that not necessarily follow the normal distributions, but their sum does follow according to the Central Limit Theorem.\n",
    "\n",
    "## 3.6 Kernel density estimates \n",
    "\n",
    "In some instances, we may not be interested in the parameters of a particular distribution of data, but just a **continous representation** of the data at hand. In this case, we can estimate the distribution non-parametrically (i.e. making no assumptions about the form of the underlying distribution) using kernel density estimation.\n",
    "\n",
    "Several uses are defined:\n",
    "\n",
    "- **Visualization** - to explore the data by visualizing them and decide whether an estimated PDF is an appropriate model for the distribution.\n",
    "\n",
    "- **Interpolation** - if we have reasons to beleive that the distribution is smooth, we can apply the KDE to interpolate the density specially for values that were not sampled.\n",
    "\n",
    "- **Simulation** - specially when the sample distribution is small, it would be convenient to smooth the sample distribution by KDE in order to simulated and explore more possible outcomes, rather than replicating the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import norm\n",
    "\n",
    "# Some random data\n",
    "y = np.random.random(15) * 10\n",
    "x = np.linspace(0, 10, 100)\n",
    "# Smoothing parameter\n",
    "s = 0.4\n",
    "\n",
    "# Calculate the kernels\n",
    "kernels = np.transpose([norm.pdf(x, yi, s) for yi in y])\n",
    "\n",
    "plt.plot(x, kernels, 'k:')\n",
    "plt.plot(x, kernels.sum(1))\n",
    "plt.plot(y, np.zeros(len(y)), 'ro', ms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy implements a Gaussian KDE that automatically chooses an appropriate bandwidth. Let's create a bi-modal distribution of data that is not easily summarized by a parametric distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bi-modal distribution with a mixture of Normals.\n",
    "x1 = np.random.normal(0, 3, 100) # parameters: (loc=0.0, scale=1.0, size=None)\n",
    "x2 = np.random.normal(8, 1, 50)\n",
    "\n",
    "# Append by row\n",
    "x = np.r_[x1, x2] # r_ Translates slice objects to concatenation along the first axis.\n",
    "\n",
    "\n",
    "plt.hist(x, bins=8, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kde\n",
    "\n",
    "density = kde.gaussian_kde(x)\n",
    "xgrid = np.linspace(x.min(), x.max(), 100)\n",
    "plt.hist(x, bins=18, density=True)\n",
    "plt.plot(xgrid, density(xgrid), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "## 4 Estimation\n",
    "* 4.1 Sample mean\n",
    "* 4.2 Variance\n",
    "* 4.3 Standard scores\n",
    "* 4.4 Covariance\n",
    "* 4.5 Pearson,s correlation\n",
    "* 4.6 Spearman's rank correlation\n",
    "<font\\> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think of a sequence of values: \n",
    "[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138].\n",
    "\n",
    "Can you guess which is the distribution? For example what would be its mean?\n",
    "\n",
    "Hint: assume that it is normal distribution.\n",
    "\n",
    "\n",
    "**Definition:** *Estimation* is the process of inferring the parameters (e.g. mean) of a distribution from a statistic of samples drown from a population.\n",
    "\n",
    "For example: What is the estimated mean $\\hat{\\mu}$ of the following normal data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0.0, 1.0, 10000)\n",
    "a = plt.hist(x,50,density='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our definition of empirical mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The empirical mean of the sample is ', x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us imagine that we were reported the following data, where probably one of the data is wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([-0.441, 1.774, -0.101, -1.138, 2.975, -213.8])\n",
    "print(x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the mean estimator good enough?\n",
    "\n",
    "### 4.1 Sample mean\n",
    "\n",
    "+ The process is called **estimation** and the statistic we used **estimator**.\n",
    "\n",
    "+ The median is also an estimator (more robust to outliers). \n",
    "\n",
    "+ \"Is median better than sample mean?\" is a question with at least two different answers. We can use two different objectives to answer this question: the minimization of error or the maximization to get the right answer. \n",
    "\n",
    "+ If there are no outliers, we can use the **sample mean** to minimize **mean squared error** (where $m$ is the number of times you play the estimation game, not the size of the sample!):\n",
    "\n",
    "$$ MSE = \\frac{1}{m} \\sum(\\hat{\\mu} - \\mu)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 0.0\n",
    "mu=0.0\n",
    "NTests=1000\n",
    "var=1.0\n",
    "NPoints=100000\n",
    "for i in range(NTests):\n",
    "    x = np.random.normal(mu, var, NPoints)\n",
    "    err += (mu - x.mean())**2\n",
    "\n",
    "print('MSE: ', err/float(NTests) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Variance\n",
    "\n",
    "We can also estimate the variance with:\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_i (x_i - \\mu)^2 $$\n",
    "\n",
    "This estimator works for large samples, but it is biased for small samples. We can use this one:\n",
    "\n",
    "$$ \\hat{\\sigma}^2_{n-1} = \\frac{1}{n-1} \\sum_i (x_i - \\mu)^2 $$\n",
    "\n",
    "\n",
    "### 4.3 Other concepts: Standard scores\n",
    "\n",
    "$$ z_i = \\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "This measure is dimensionless and its distribution has mean 0 and variance 1.\n",
    "\n",
    "It inherits the \"shape\" of $X$: if it is normally distributed, so is $Z$. If $X$ is skewed, so is $Z$.\n",
    "\n",
    "### 4.4 Covariance (optional)\n",
    "\n",
    "Sometimes it would be of interest to measure the relationship between two variables. \n",
    "\n",
    "**Covariance** is a measure of the tendency of two variables to vary together. \n",
    "\n",
    "If we have two series $X$ and $Y$ with $X=\\{x_i\\}$ and $Y=\\{y_i\\}$, and they vary together, their deviations $x_i - \\mu_X$ and $y_i - \\mu_Y$ tend to have the same sign.\n",
    "\n",
    "If we multiply them together, the product is positive, when the deviations have the same sign, and negative, when they have the opposite sign. So adding up the products gives a measure of the tendency to vary together.\n",
    "\n",
    "Covariance is the mean of the products:\n",
    "\n",
    "$$ Cov(X,Y) = \\frac{1}{n} \\sum (x_i - \\mu_X)*(y_i - \\mu_Y), $$\n",
    "\n",
    "where $n$ is the length of the two series.\n",
    "\n",
    "It is a measure that is difficult to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cov(X, Y):\n",
    "    def _get_dvis(V):\n",
    "        return [v - np.mean(V) for v in V]\n",
    "    dxis = _get_dvis(X)\n",
    "    dyis = _get_dvis(Y)\n",
    "    return np.sum([x * y for x, y in zip(dxis, dyis)])/len(X)\n",
    "\n",
    "X = [5, -1, 3.3, 2.7, 12.2]\n",
    "Y=[10,12,8,9,11]\n",
    "\n",
    "print(\"Cov(X, X) = %.2f\" % Cov(X, X))\n",
    "print(\"Var(X) = %.2f\" % np.var(X))\n",
    "\n",
    "print(\"Cov(X, Y) = %.2f\" % Cov(X, Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/gasolineprice.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Pearson's Correlation\n",
    "\n",
    "Shell we take into account the variance? An alternative is to divide the deviations by $\\sigma$, which yields standard scores, and compute the product of standard scores:\n",
    "\n",
    "$$ p_i = \\frac{(x_i - \\mu_X)}{\\sigma_X} \\frac{(y_i - \\mu_Y)}{\\sigma_Y} $$\n",
    " \n",
    "The mean of these products is:\n",
    "\n",
    "$$ \\rho = \\frac{1}{n} \\sum p_i = \\frac{1}{n} \\sum  \\frac{(x_i - \\mu_X)}{\\sigma_X} \\frac{(y_i - \\mu_Y)}{\\sigma_Y}  $$\n",
    "\n",
    "Or we can rewrite $\\rho$ by factoring out $\\sigma_X$ and $\\sigma_Y$:\n",
    "\n",
    "$$ \\rho = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corr(X, Y):\n",
    "    assert len(X) == len(Y)\n",
    "    return Cov(X, Y) / np.prod([np.std(V) for V in [X, Y]])\n",
    "\n",
    "print(\"Corr(X, X) = %.5f\" % Corr(X, X))\n",
    "\n",
    "Y=np.random.random(len(X))\n",
    "\n",
    "print(\"Corr(X, Y) = %.5f\" % Corr(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/pearson.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\rho = 0$, we cannot say that there is no relationship between the variables!\n",
    "\n",
    "Pearson's coefficient only measures **linear** correlations!\n",
    "\n",
    "### 4.6 Spearman‚Äôs rank correlation\n",
    "\n",
    "Pearson‚Äôs correlation works well if the relationship between variables is linear and if the variables are roughly normal. But it is not robust in the presence of **outliers**.\n",
    "\n",
    "Spearman‚Äôs rank correlation is an alternative that mitigates the effect of outliers and skewed distributions. To compute Spearman‚Äôs correlation, we have to compute the rank of each value, which is its index in the sorted sample. \n",
    "\n",
    "For example, in the sample {7, 1, 2, 5} the rank of the value 5 is 3, because it appears third if we sort the elements. \n",
    "\n",
    "Then, we compute the Pearson‚Äôs correlation, **but for the ranks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2rank(l):\n",
    "    #l is a list of numbers\n",
    "    # returns a list of 1-based index; mean when multiple instances\n",
    "    return [np.mean([i+1 for i, sorted_el in enumerate(sorted(l)) if sorted_el == el]) for el in l]\n",
    "\n",
    "l = [7, 1, 2, 5]\n",
    "print(\"ranks: \", list2rank(l))\n",
    "\n",
    "def spearmanRank(X, Y):\n",
    "    # X and Y are same-length lists\n",
    "    return Corr(list2rank(X), list2rank(Y))\n",
    "\n",
    "X = [1, 2, 3, 4, 100]\n",
    "Y = [5, -100, 7, 10, 9]\n",
    "plt.plot(X,'ro')\n",
    "plt.plot(Y,'go')\n",
    "\n",
    "print(\"Pearson rank coefficient: %.2f\" % Corr(X, Y))\n",
    "print(\"Spearman rank coefficient: %.2f\" % spearmanRank(X, Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Obtain for the Anscombe's quartet, the different estimators (mean, variance, covariance for each pair, Pearson's correlation and Spearman's rank correlation.\n",
    "\n",
    "(Source: http://en.wikipedia.org/wiki/Anscombe's_quartet):\n",
    "\n",
    "<center><img src=\"images/Anscombe's_quartet.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Show if there is a correlation between the data of the babies...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution of the Messi's exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 10000 # perform N experiments\n",
    "M = 0 # no of times we get one 6\n",
    "T = 42\n",
    "Messi_score=0.83\n",
    "Cristiano_score=0.62\n",
    "success='1111111111'\n",
    "\n",
    "for i in range(N):\n",
    "    outcome=''\n",
    "    for i in range(T):\n",
    "        res=random.random()\n",
    "        if res<Cristiano_score:\n",
    "            outcome=outcome+'1'\n",
    "        else: outcome=outcome+'0'\n",
    "\n",
    "    if (success in outcome):\n",
    "            M += 1\n",
    "\n",
    "print('Cristiano got scores at least 1 goal in a row of 10 matches during a season %d times out of %d' % (M, N), \n",
    "      '; Prob = ', float(M)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main reference\n",
    "\n",
    "*Think Stats: Probability and Statistics for Programmers*, by Allen B. Downey, published by O'Reilly Media.\n",
    "\n",
    "http://www.greenteapress.com/thinkstats/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
